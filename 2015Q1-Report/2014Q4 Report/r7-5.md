## WBS 22.5 Computing Systems and Infrastructure

During the LS1 shutdown period the work within CMS Computing
Infrastructure and Services centers around infrastructure improvements
needed to be fully ready for the next LHC run in 2015. Work in this area
includes improving the functionality, stability and scalability of the
grid and cloud based job submission infrastructure, enabling the use of
opportunistic resources, and improving the availability of the now
dozens of petabytes of CMS data through more dynamic and flexible
storage strategies.

This quarter saw a special emphasis on putting into production
improvements related to multi-threading, allowing CMS to take better
advantage of modern CPU architectures. For Workload Management, this has
the added benefit of reducing the number of jobs that must be tracked in
the system since each job can do more work. For Run2 CMS will switch
many production jobs to our multi-threaded framework and the workflow
system needed some changes to cope with this.

In 2014 CMS divided computing infrastructure tasks into “Workload
Management” and “Data Management” categories. USCMS holds important
management and development responsibilities within both. This section of
the report is structured accordingly.

Workload Management

CMS Workload Management is coordinated by Eric Vaandering. Tasks under
this umbrella, which USCMS supports, include the WMAgent production
workload system and the Tier 0 system (based on the WMAgent). This
effort also includes completing the CRAB3 analysis workload system. The
HTCondor-based glideinWMS provides the job submission infrastructure for
the entire CMS experiment, which is also the primary stakeholder of the
program. GlideinWMS development was coordinated by Burt Holzman through
2014; coordination will shift to Parag Mhashilkar of FNAL in 2015. 
Commissioning and integration of a global pool of resources across production
and analysis workloads is coordinated as part of the "CMS Submission Infrastructure” group
by James Letts.

WMAgent

The WMAgent system manages all CMS production workloads. The lead
developer is Seangchan Ryu. To address ongoing problems while making
progress on new improvements, the development effort has been divided
into two teams: A core development team, which focuses on adding the new
functionality and features needed for Run 2, and a “firefighting” team,
which handles triage of issues encountered in daily operations and
handling any necessary fixes to keep work moving. Being closer to the
operations themselves, the “firefighting” team also handle some
integration of new features.

This quarter saw major improvements to the software to support
scheduling and running multi-threaded applications on the Grid. A large
amount of work was also performed to prepare for the next iteration of
our aging workflow request system and to support, as detailed in the
data management section, the direct association of request metadata with
the dataset(s) produced in those requests. Finally, from a code
improvement standpoint, the ability to accumulate physics-quantity
monitoring statistics over multiple runs was added now that the
monitoring system supports such functionality.

Another campaign of work over the quarter was needed to bring WMAgent up
to date with the underlying software and operating systems we depend on.
Work was done (and is ongoing) to work with the latest versions of the
Apache web server and CouchDB NoSQL database server. This quarter we
have finished the work needed to deploy WMAgent on Scientific Linux 6
(SL6). The majority of WMAgent instances are now deployed on SL6.
Progress has also been made in moving deployment procedures to puppet,
allowing CMS to quickly deploy agents as needed as additional resources
become available – for example in the case of CMS acquiring
opportunistic or allocation based computing resources.

Tier 0

Under lead developer Dirk Hufnagel, this quarter has brought new
developments to the Tier0 which are shaken down on increasingly complex
CMS data taking exercises called “global runs”. Tier0 replays of Run1
collision data are also tested at increasing scale with the CERN Agile
(virtual machine) infrastructure which will be used in Run2. The Tier0
is based on WMAgent and also uses GlideinWMS, but some customization is
needed to support the unique Tier0 workflow.

During this quarter, additional development for the Tier0 was needed to
fully enable execution of multi-threaded workflows. This has thoroughly
tested in the Tier0 but not yet in production since the final version of
the CMSSW application is not validated for physics performance yet. This
task ran into some difficulties related to concentration of special,
high I/O, tasks on a limited number of virtual machines, but this has
been remedied this quarter.

In Run2 CMS will offload a large fraction of our prompt reconstruction
from the Tier0 to the Tier1. The development for this was completed this
quarter and as our Tier1 sites provide more multi-core batch resources,
this will be scale tested at increasing levels. Finally, the monitoring
portion of WMAgent (WMStats) was extended to adequately monitor the
operations of the Tier0.


CRAB3

In 2013 USCMS stepped forward in order to complete the CRAB3 project in
time for Run 2, and for three months this summer, the Computing,
Software and Analysis ’14 (CSA14) challenge allowed CMS physicists to
test the CMS software suite on simulated Run 2 data. During this
challenge over 200 users used the CRAB3 infrastructure to analyze over a
billion events held in over 1600 physics samples. The challenge was
successful, in that considerable experience with and user feedback on
the CRAB3 system was obtained. During Q4 two additional workflows, Monte
Carlo generation from LHE files and the ability to run an arbitrary
script, were added. Additional scale tests were performed and
development related to hardening and improving the systems were also
completed. A transition plan has been defined and has begun to be
implemented: CRAB3 has been moved into production and will be the only
supported analysis workflow system by the start of Run2.


GlideinWMS Development, Support, and Integration

The scope of the GlideinWMS program extends beyond the CMS VO, however
CMS is its primary stakeholder. The HTCondor and GlideinWMS submission
infrastructure form the backbone of all distributed processing, both
analysis and central production and processing. The GlideinWMS team
continues to provide releases at an accelerated rate as we move towards
Run2. During the reporting period, two releases were prepared by the
GlidinWMS team. In October, 3.2.7 included shared port support and
allowed monitoring information to be sent to a separate collector. The
shared port support will help CMS utilize the CERN OpenStack based
virtual machines and increase the scale of operations more effectively,
and the monitoring improvements will help with aggregating monitoring
across several factories and schedd’s, providing a better monitoring
overview of the system. In December, version 3.2.8 was released with
improved monitoring for multicore pilots, which will allow CMS to
understand the operational (in)efficiencies associated with operating
multicore pilots with a mix of multi- and single-threaded applications.
Version 3.2.8 also improved the scalability of GlideinWMS, enabling CMS
to run and schedule more jobs across the entire grid system. 

These improvements are important as CMS is in the midst of commission a single
global HTCondor/glideinWMS pool, combining analysis and central
production as well as push forward with multi-threaded applications.
Within Q1 of FY 2015, initial operations of this global pool commenced
with the expectation that the transition to it is complete by the time Run2 starts.
CMS provides scalability targets for HTCondor, and otherwise 
relies on OSG for testing and documentation of HTCondor scalability.
During Q1 of FY 2015, all the CMS scalability goals for HTCondor have been
met. The remaining step before beginning of Run2 is now to transfer knowledge,
and integrate lessons learned into the CMS production infrastructure.

Opportunistic and Cloud Computing

CMS production continues to use the HLT cloud and the OpenStack Agile
Infrastructure at CERN. This quarter work has focused on enabling the
NERSC Carver and SDSC Gordon allocations for CMS production use. Using
the BOSCO resource support added to GlideinWMS, test jobs were
successfully executed at SDSC Gordon. A similar approach was attempted
at NERSC Carver, but also using Parrot  to bring in the CMS software
environment. However, a bug in Parrot on GPFS was discovered which must
be fixed by the Parrot developers before tests can resume. When this is
completed, it will also allow staging of produced data to CMS owned
resources. We expect to be able to use both of these allocations in
early 2015.

To use the HLT effectively during the inter-fill period, the time to
fill the HLT with work must be minimized and the HLT must be vacated
with 15 minutes warning that CMS is returning to collision data taking.
To accomplish this, the team is investigating long-lived virtual
machines on the HLT that can be easily suspended.


Data management

Coordination of CMS Data Management activities is shared by USCMS (Tony
Wildish) and CERN. Services USCMS are responsible for include the Data
Bookkeeping Service, Data Aggregation Service, and the PhEDEx data
transfer system. Ongoing work continues in the Any Data, Anytime,
Anywhere project and Dynamic Data Placement.


DBS

The CMS Data Bookkeeping Service, with lead developer Yuyi Guo,
bookkeeps all CMS production (and much analysis) data in an Oracle
database that maps all produced files and relevant metadata to their
associated datasets. DBS version 3 has been successfully in operation
since spring of 2014. Effort this quarter has been focused on migrating
the DBS servers from aging SL5 nodes at CERN to SL6. Some development
for DBS continues to be needed for new use cases, but this effort is
ramping down; DBS is expected to be mostly in maintenance mode in 2015.
Developments this quarter include using DBS as the source of the
relationship between produced data and the databases which store
production request information, providing APIs useful for our data
mining efforts, and enhancing the debugging abilities for our web
services.


DAS

The Data Aggregation Service, lead developer Valentin Kuznetsov,
provides a queryable glue between the different data services in CMS,
including DBS, PhEDEx, and the online run summary database. Work in DAS
this quarter includes the ability to scale DAS across multiple servers
and adapting to the addition of requested APIs in DBS, including those
relating datasets and production requests. Work will continue toward
further optimizing queries and providing the necessary hooks to retrieve
needed data from metadata spanning the multiple services.


PhEDEx

Under lead developer Tony Wildish, PhEDEx continues to be a stable
service, driving the bulk data transfers for all the dozens of CMS data
centers. This quarter brings a new PhEDEx release including the
capability to perform data move subscriptions to disk based resources.
Formerly only allowed to custodial tape resources, disk moves are
necessary for CMS operations to better be able to more dynamically take
advantage of the Tier 1 resources, and increase our ability to scale
test smaller sites without having to “burn” precious tape storage.
Multi-step workflows may be run without having to re-stage custodial
data off of tape to perform the next step, as was often needed with the
old more restrictive move requests. Scale tests may also now be run with
a simultaneous move request active, which allows testing of sites at
higher scales than before, especially Tier 2 sites with limited disk
space. Additional work this quarter has focussed on the space monitoring
system which is associated with, but not technically part of, PhEDEx.
Finally changes have been made to PhEDEx as part of a CMS-wide campaign
to improve how we designate storage locations. This quarter we also
began using PhEDEx to transfer the log files produced by our workflow
system.


AAA

“Any Data, Anytime, Anywhere” is a separately funded project which aims
to provide data as its name implies. The capability involves serving all
disk resident CMS data through xrootd, enabling its access from any
network accessible location. Tests of AAA continued throughout this
quarter, testing file availability and the rate at which files can be
opened through remote access. The CSA14 exercise brought the xrootd read
capabilities developed by the AAA project to the masses, where physicists
learned to be able to access any sample that’s available on disk based
resources in CMS. This is revolutionizing how CMS analyzers do physics
analysis by allowing them to use available cpu resources irrespective of where datasets are staged in.
Scale testing of AAA and xrootd was augmented this quarter by
“overflowing” analysis jobs to available CPU while forcing remote data
reads via AAA. Central production also began commissioning xrootd reads
of input data for reprocessing. In addition, CRAB3 allows the physics community
to explicitly ignore where data is staged. Physicists thus can choose to use AAA
in order to use CPU resources at sites irrespective of whether or not those sites have the data locally staged.
The main remaining development activity in AAA that has not yet been slated for integration into the production
system is the XRootd proxy-cache. The caching software allows partial file reads to be cached automatically.
It’s first use case is likely the deployment at T3s later in 2015. Production deployment is not
considered a must have functionality for the beginning of Run2.

Dynamic Data Management (DDM)

This project was transitioned from Christoph Paus’s leadership to be led
by Maxim Goncharov in the future. DDM uses the CMS data popularity
service to manage data replicas throughout CMS’s tiered storage systems.
This system has been in place since May 2014 and is operated from MIT.
In the past quarter all Tier2 storage systems were brought under the DDM
umbrella and the FNAL Tier1 was incorporated as well. Tier1 policies
vary, so each Tier1 must be treated individually.

Development on the DDM project progressed this quarter with an overhaul
of the algorithm used to create additional copies of popular data; all
indications are that this change is a significant improvement.
Monitoring for this component has also been improved. A new component of
the system was developed to deal with sites which become unavailable,
replicating their data elsewhere if sites are down for extended periods
of time. If the site returns to service, it is re-integrated into the
system. Finally, overall monitoring of space usage has been improved to
help CMS understand how the efficiently the CMS storage is used across
the system.


