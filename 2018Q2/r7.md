[Summary of Software and Computing activities]{.c7} {#h.8gule4ziae3r .c12}
===================================================

[]{.c0}

The second quarter of 2018 saw the start of the final year of LHC
operations before Long Shutdown 2, and a major push within CMS to
complete all analysis of data recorded in 2016 in preparation for summer
conferences.  CMS data-taking currently includes about 2 kHz[ of
"parked" data, which will not be reconstructed until after the run.  The
triggers for the parked datasets are designed to capture b hadron
production, to address anomalies observed in the B system.  This has
gone smoothly, along with the rest of data taking, and the general
operations of the distributed infrastructure as a whole.  As usual,
these operations were supported by the strong performance of the U.S.
computing facilities, whose utilization has been as high as ever.  The
facilities did have an unusual situation to contend with: transitions
within the Open Science Grid organization that have led to changes in
operations and support.  Overall this transition has been managed with
very little disruption of the sites.  In addition, Fermilab chose a
vendor for a new tape archive, which will be deployed in the coming
months.  The development areas made progress on a number of fronts.
 Usage of NERSC resources in production mode was ramped up.  A review of
workflow management systems was kicked off, and preparations were made
for a data management review to be held in July.  Pre-mixed pileup
samples are now useable for HL-LHC simulations, a major step that will
speed the production of samples for physics studies.  Revised estimates
of HL-LHC computing needs give us a better understanding of the
innovations that will be needed to sustain the future physics
program.]{.c0}

[]{.c0}

[Major milestones achieved this quarter]{.c7}

[]{.c0}

[]{#t.941978613a23cc3d0e9645320d4d12d40fc8968e}[]{#t.0}

+-----------------------------------+-----------------------------------+
| [Date]{.c0}                       | [Milestone]{.c0}                  |
+-----------------------------------+-----------------------------------+
| [10 April 2018]{.c0}              | [Preliminary report on Rucio      |
|                                   | investigation delivered.]{.c0}    |
+-----------------------------------+-----------------------------------+
| [1 April 2018]{.c0}               | [Demonstrate use of HTTP as       |
|                                   | replacement to GridFTP at two     |
|                                   | Tier-2 sites.]{.c19}              |
+-----------------------------------+-----------------------------------+
| [10 May 2018]{.c0}                | [CMS Workflow Management Review,  |
|                                   | Part 1]{.c0}                      |
+-----------------------------------+-----------------------------------+
| [1 June 2018]{.c0}                | [Evaluate performance metrics on  |
|                                   | CPUs for Machine Learning         |
|                                   | Benchmark 1]{.c0}                 |
+-----------------------------------+-----------------------------------+
| [15 June 2018]{.c0}               | [CRAB3 able to trigger orderly    |
|                                   | recall from archive.]{.c0}        |
+-----------------------------------+-----------------------------------+
| [30 June 2018]{.c0}               | [First full implementation of     |
|                                   | pre-mixing for HL-LHC MC PileUp   |
|                                   | Digitization]{.c0}                |
+-----------------------------------+-----------------------------------+
| [30 June 2018]{.c0}               | GPU vectorized tracking - Working |
|                                   | track building version for        |
|                                   | simplified geometry and           |
|                                   | Monte-Carlo simulated events      |
+-----------------------------------+-----------------------------------+

[]{.c7} {#h.98q9s5c6ll0u .c11}
=======

[Fermilab Facilities ]{.c7} {#h.w7cgrp7uowuj .c12}
===========================

[]{.c0}

[Q2 of 2018 covered the restart of LHC physics operations and large
scale data processing in preparation of physics results for upcoming
summer conferences.  Throughout this quarter the Fermilab Facilities
continued to provide reliable custodial storage, processing and analysis
resources to U.S. CMS collaborators.  The site was well utilized, with
the facility providing 43.4 million wall-clock hours of processing to
CMS.  ]{.c0}

[![](images/image5.png)]{style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 274.40px;"}

[]{.c0}

[Figure 1 shows the site readiness metrics for the quarter. During this
quarter the Tier 1 facility passed CMS site availability metrics 98.0%
of the time.  In the beginning of the quarter, prior to LHC operations,
all storage machines were rebooted, and a handful developed hardware
problems afterwards, which resulted in lower data transfer efficiencies
and the red metrics visible in April.  We worked with the hardware
vendor to provide backup servers onsite to prevent similar problems from
happening in the future.]{.c0}

[]{.c0}

[During this quarter Fermilab reviewed proposals for new tape libraries
and made the decision to move to an IBM TS4500 tape library with LT08
media to replace the current aging Oracle SL8500 libraries.  Work has
begun to prepare for delivery of the CMS library early next
quarter.]{.c0}

[]{.c0}

[University Facilities]{.c7} {#h.1n7rsuf66o15 .c12}
============================

[]{.c0}

[As seen in Figure 2, CMS production and analysis activities this
quarter continued to run at full capacity and even exceeded purchased
processing power, taking advantage of the sizable opportunistic
processing available at the U.S. CMS Tier-2 sites. During this period of
heavy usage, analysis processing consumption by U.S. physicists
continued at the level of 68% of the total analysis CPU delivered by our
sites during the last quarter.]{.c0}

[]{.c0}

All of the U.S. CMS Tier-2 sites operated successfully last quarter. On
our two official performance metrics based on CMS test jobs, all sites
were at least 97%
"[[available](https://www.google.com/url?q=http://wlcg-sam-cms.cern.ch/templates/ember/%23/historicalsmry/heatMap?end_time%3D2018%252F07%252F01%252000%253A00%26granularity%3DDaily%26profile%3DCMS_CRITICAL%26site%3DT2_US_Caltech%252CT2_US_Florida%252CT2_US_MIT%252CT2_US_Nebraska%252CT2_US_Purdue%252CT2_US_UCSD%252CT2_US_Wisconsin%26start_time%3D2018%252F04%252F01%252000%253A00%26time%3Dmanual%26type%3DAvailability%2520Ranking%2520Plot&sa=D&ust=1532360716914000){.c6}]{.c16}"
and 90%
"[[ready](https://www.google.com/url?q=https://dashb-ssb.cern.ch/dashboard/request.py/sitereadinessrank?columnid%3D234%23time%3Dcustom%26start_date%3D2018-04-01%26end_date%3D2018-07-01%26sites%3Dmultiple%26timebins%3Dfalse%26nodata%3Dfalse%26binsselect%3Ddefault%26clouds%3Dall%26site%3DT2_US_Caltech,T2_US_Florida,T2_US_MIT,T2_US_Nebraska,T2_US_Purdue,T2_US_UCSD,T2_US_Wisconsin&sa=D&ust=1532360716914000){.c6}]{.c16}[".
The CMS requirement for each of these metrics is 80%, but the U.S. CMS
performance goal is 90%, which all sites met.]{.c0}

[]{.c0}

The U.S. CMS Tier-2 centers delivered
[[48.1%](https://www.google.com/url?q=http://dashb-cms-jobsmry.cern.ch/dashboard/request.py/consumptions_individual?sites%3DT2_AT_Vienna%26sites%3DT2_BE_IIHE%26sites%3DT2_BE_UCL%26sites%3DT2_BR_SPRACE%26sites%3DT2_BR_UERJ%26sites%3DT2_CH_CSCS%26sites%3DT2_CN_Beijing%26sites%3DT2_DE_DESY%26sites%3DT2_DE_DESY_Test%26sites%3DT2_DE_RWTH%26sites%3DT2_EE_Estonia%26sites%3DT2_EE_Estonia_Test%26sites%3DT2_ES_CIEMAT%26sites%3DT2_ES_IFCA%26sites%3DT2_FI_HIP%26sites%3DT2_FI_HIP_Test%26sites%3DT2_FR_CCIN2P3%26sites%3DT2_FR_GRIF_IRFU%26sites%3DT2_FR_GRIF_LLR%26sites%3DT2_FR_IPHC%26sites%3DT2_GR_Ioannina%26sites%3DT2_HU_Budapest%26sites%3DT2_IN_TIFR%26sites%3DT2_IT_Bari%26sites%3DT2_IT_Legnaro%26sites%3DT2_IT_LegnaroTest%26sites%3DT2_IT_Pisa%26sites%3DT2_IT_Rome%26sites%3DT2_KR_KNU%26sites%3DT2_MY_UPM_BIRUNI%26sites%3DT2_PK_NCP%26sites%3DT2_PL_Swierk%26sites%3DT2_PL_Warsaw%26sites%3DT2_PT_NCG_Lisbon%26sites%3DT2_RU_IHEP%26sites%3DT2_RU_INR%26sites%3DT2_RU_ITEP%26sites%3DT2_RU_JINR%26sites%3DT2_RU_PNPI%26sites%3DT2_RU_RRC_KI%26sites%3DT2_RU_SINP%26sites%3DT2_TH_CUNSTDA%26sites%3DT2_TR_METU%26sites%3DT2_UA_KIPT%26sites%3DT2_UK_London_Brunel%26sites%3DT2_UK_London_BrunelTest%26sites%3DT2_UK_London_IC%26sites%3DT2_UK_SGrid_Bristol%26sites%3DT2_UK_SGrid_RALPP%26sites%3DT2_US_Caltech%26sites%3DT2_US_Florida%26sites%3DT2_US_MIT%26sites%3DT2_US_Nebraska%26sites%3DT2_US_Purdue%26sites%3DT2_US_UCSD%26sites%3DT2_US_Vanderbilt%26sites%3DT2_US_Wisconsin%26sitesSort%3D2%26start%3D2018-04-01%26end%3D2018-07-01%26timeRange%3Ddaily%26granularity%3DMonthly%26generic%3D0%26sortBy%3D0%26series%3DAll%26type%3Dewa&sa=D&ust=1532360716916000){.c6}]{.c16} of
all computing time by Tier-2 sites in CMS last quarter.[ This is a
further decrease of 1.2% from the previous quarter, indicating that our
pressure on CMS to diversify the geographical spread of production work
may be having a positive effect. However, given that \~70% of our U.S.
processing resources for analysis are used by local U.S. physicists, as
seen in Figure 2, and that we also take our fair share (\~30%) of
non-U.S. Tier-2 analysis usage, U.S.-based researchers are doing
approximately 50% of the global analysis work in CMS, commensurate with
our \~50% T2 resource usage overall.]{.c0}

[]{.c0}

[![](images/image4.png)]{style="overflow: hidden; display: inline-block; margin: -0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 584.00px;"}

Figure 2:  How processing resources were used at the U.S. CMS Tier-2
sites, by month.

[]{.c0}

No new previously planned Tier-2
[[milestones](https://www.google.com/url?q=https://twiki.cern.ch/twiki/bin/view/CMSPublic/USCMSTier2Upgrades?rev%3D316&sa=D&ust=1532360716917000){.c6}]{.c16} were
completed during the last quarter. However, steady progress is being
made on many fronts, especially for data storage and transfer
milestones. The University Facilities were largely focused on the OSG
transition during the quarter, especially migrating the ticketing system
to GGUS and preparing for the end of life of the OSG certificate
authority.

[]{.c0}

The U.S. CMS Tier-3 support team provided help to six sites on a number
of issues.  The biggest concern was the transition away from the OSG
GOC.  For Tier-3 sites, the two main impacts were in the procedures for
obtaining host certificates with the OSG certificate authority ending,
and the closing of the GOC ticketing website and transition to GGUS
tickets.  In addition, routine user support for CMS Connect was
provided, as well as integration work for the Tier-3 in a Box program
and some automation for the central Tier-3 PhEDEx service.

[Computing Operations]{.c18} {#h.ri71pzuh5u31 .c8}
============================

[Digitizing and simulating Monte Carlo events for the 2017 data analyses
was the largest processing activity in the quarter. Close to 10 billion
events were processed, about 2/3rd of the requested Monte Carlo events.
All processing used the resource-efficient, pre-mixing pile-up method
and honored the requested priorities.]{.c0}

[]{.c0}

[Monte Carlo generation for the 2018 detector configuration started in
the quarter. A small 0.9 billion pre-production sample was created.
Requests for the full production campaign are currently being submitted,
1.7 billion events so far.]{.c0}

[]{.c0}

[The Tier-0 reformatted and reconstructed data from the detector as they
were recorded.  The baseline trigger rate remains 1 kHz, but there are
now additional triggers for B physics data that are not processed
directly but stored for parking, adding about another 2 kHz of total
trigger rate. While the Tier-0 is dealing with those rates without real
problems as of yet they push the system into a more busy state. The LHC
is running very well and luminosity delivery about two weeks ahead of
schedule. Distribution of RAW data to the Tier-1 sites is keeping up.
Transfers to the largest Tier-1, the U.S. site, fell behind for a few
weeks. The bottleneck was identified to be in the metadata handling of
an old transfer component and eliminated. Calibration and offline
reconstruction of the 2018 data is also keeping up.]{.c0}

[]{.c0}

[Since the middle of May processing has been limited by available
computing resources. Including Tier-0 resources at CERN which are now
part of the global CMS resource pool, between 200k (beginning of the
quarter) and 300k cores (end of the quarter) were used. Processing
requests exceed available resources and only high priority requests are
currently executed. Analysis load has been constant at about 50k cores
during the quarter.]{.c0}

[![](images/image1.png)]{style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 341.33px;"}

[]{.c0}

[]{.c0}

[New probes for the WLCG site evaluation framework, SAM, were deployed
and integrated into the performance metric. The roll out of Singularity
was completed before data taking started and urgent updates, due to
discovered security vulnerabilities, handled.]{.c0}

[]{.c13}

[Computing Infrastructure and Services]{.c7} {#h.3z769mydqy3h .c12}
============================================

[]{.c0}

In this quarter, the WMAgent production workflow system began the
improvements necessary to finish transition of "StepChain" into
production; the last pieces needed are improvements to the metadata
registered with the Data Bookkeeping System (DBS) when workflows are run
in this mode.  We expect this to be finished next quarter.  The job
logging improvements begun last quarter have been deployed and are
beginning to be integrated in the higher layers, allowing operators to
more easily retrieve log files of failed jobs.  A first version of the
"fast draining" patch (allowing improved operational efficiencies) has
been finished and is expected to be deployed next quarter.  We have
begun an external review of the future plans of the CMS Workflow
Management system; we believe this will be completed with a committee
report in one to two quarters.

[]{.c0}

Use of NERSC-based resources in production mode ramped up this quarter.
 This work heavily utilized the HEPCloud effort within the CIAS area.
 We have integrated this resource into the CMS "global pool", meaning
the production system can run workflows into NERSC without any special
setup (such as partitioning the workflows to [only]{.c1} run at NERSC).
 This has been a significant step in the resource's evolution and will
allow us to turn our focus to other HPC resources next quarter.  Along
with the usage of OSG-based opportunistic resources, this has allowed us
to deliver significant value to the CMS experiment beyond traditional
WLCG resources.  The figures below illustrates the activity over the
last month:

[![](images/image2.png)]{style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 300.47px; height: 190.50px;"}[![](images/image3.png)]{style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 300.47px; height: 190.50px;"}

[]{.c0}

[The evaluation of Rucio as a future data management system is
proceeding toward the data management review in the next quarter.  We
have demonstrated the ability to include several additional sites in the
Rucio testbed and the ability to do CMS-style generation of transfer
URLs (necessary for a smooth transition to production). For analysis
users, the Dynamo system has been modified to allow the orderly staging
of datasets when a CRAB job requests a dataset that is on tape
only.]{.c0}

[]{.c0}

For data management through caching, we have added support of direct
access for fully downloaded files when XCache is running on a shared
file system (such as Lustre, as requested by NERSC). The latest release
has also improved open request processing, allow higher open rate needed
to support higher levels of activity at UCSD.  Finally, the
collaboration with UCSC continues through studies of CMS data access
patterns.

[]{.c0}

[Software and Support]{.c7} {#h.d5b286qlnfo1 .c12}
===========================

[]{.c0}

[In the second quarter of calendar year 2018, data taking commenced at
the LHC for the last year of the Run 2 period. The software teams
released the main 2018 data taking and simulation release, as well as
releases for simulation and re-processing of analysis samples (MINIAOD)
of 2016 data. Special support was needed for the high β\* test running
of the LHC.  ]{.c0}

[]{.c0}

[We achieved a major milestone in finishing the first complete
implementation of pre-mixing of pileup for HL-LHC MC studies, enabling
us to produce HL-LHC simulation at all sites and not only special sites
with extraordinary high I/O capabilities. We achieved several minor
milestones from improving the threading efficiency of data quality
monitoring plots in the framework, to finalizing the configurations to
build CMSSW with cmake, to achieving an order of magnitude reduction in
error and warning log messages from the application. ]{.c0}

[]{.c0}

[Our investment in the C++ standard and the community, C++ modules, was
integrated into CMSSW. This reduces both memory consumption and speed of
building a release and allows us to validate releases more quickly and
efficiently.  We are also leading the community solution Frontier to
serve alignment and calibration constants to all CMS applications.]{.c0}

[]{.c0}

[We made progress in using the free tier of the commercial caching
provider Cloudflare and are using it now for LHC\@Home workflows
(running on Boinc like SETI\@Home). This will be used for the OpenData
project in the future, providing access for analysis of CMS public data
for everyone. We are investigating the evolution of the EVE event
display solution for ROOT 7, which will transition to web-based graphics
solution. A prototype of a client-server implementation was developed
and is being tested.  We also made significant progress in developing a
machine learning benchmark to both characterize resources as well as
normalize resource requests. The other half of machine learning, the
inference of training models, is a problem of distributing a model to
the grid and using it from an application. We developed a prototype for
the most popular framework and deployed it on a testbed for everyone to
evaluate: TensorFlow-as-a-Service]{.c0}

[]{.c0}

[The R&D for HL-LHC continues together with our partners. We tested ROOT
I/O improvements on Cori and Cori II, and in general improved the
interface of the CMSSW framework to use accelerators like GPUs.  The
Joint NSF/Operations Program team investigating vectorized tracking took
part in the 3rd Patatrack hackathon at CERN to better understand the
parallel kalman filter algorithm mkFit on GPUs and to investigate new
profiler metrics to develop a performance model on GPUs. We are
profiling mkFit on new 2018 HLT nodes (Intel Skylake architecture with
64 HT cores) with very encouraging results investigating to run the
first iteration of tracking (pixel seed and inside-out pattern
recognition) on level 1 triggered events. Next quarter, we will report
on feedback from many CHEP presentations with contributions from the
program.]{.c0}

[Other activities]{.c7} {#h.c25nhoz4lslv .c8}
=======================

[]{.c0}

The U.S. CMS cybersecurity team handled a major task, the transition
away from the OSG Certificate Authority (CA).  They worked with [all
U.S. CMS sites to renew their OSG CA certificates, and then solved
various issues before the shutdown, such as installing the new inCommon
CA, getting the new certs, checking whether new certs works with grid
resources and so on.  There were no major tickets or issues after the
shutdown.  The team worked with the Tier-2 sites to start getting
InCommon certificates.  The team also wrote a document gathering
information about job traceability and monitoring at CMS, responded to
CMS Security Officer's questionnaire about sites' cybersecurity status,
and read InCommon Certificate service API documentation.]{.c0}

[]{.c0}

[Blueprint activity accomplishments for Q2 included a major update to
the U.S. CMS projections for computing needs through the startup phase
of HL-LHC (up to 2030). Work included developing a model for analysis
resource requirements based on monitoring data, incorporating changes
due to the production and usage of NANOAOD (a newly developed data tier
in CMS that is O(1kB) per event for analysis, as well as incorporating
results from recent progress on simulation, premixing for digitization,
and reconstruction processing time improvements due to ongoing R&D.
 This update was presented at the DOE roundtable meeting in May.\
\
Subsequent work has focused on developing new model components for
HL-LHC projections. Work in progress includes cost-modeling and
projections of network requirements for both national and transatlantic
networks.  \
]{.c0}

[]{.c0}
