\newpage

# Detector Operations

Problems with the CMS magnet continued through the quarter.  Since March the Cold Box(CB) that produces	 Liquid He	for the operation of the CMS magnet	has	shown problems, after an incident where compressor oil contaminated the CB circuit. For definitive	recovery, the system	requires an	
overall cleanup	which takes several months  and will take place during the current Year End Technical Stop. Meanwhile, the CERN cryo group,	in collaboration 
with the CMS Technical Coordination, was able operate the CB with	a reasonable Duty Cycle during this quarter.    Figure \ref{fig:lumi} shows the total luminosity delivered and recorded by CMS during 2015.  Of this data, approximately 0.40 fb$^{-1}$ is with the magnetic field off or below  nominal value.

Cumulative offline luminosity versus day delivered to (blue), and recorded by CMS (orange) during stable beams and for p-p collisions at 13 TeV centre-of-mass energy in 2015. The delivered luminosity accounts for the luminosity delivered from the start of stable beams until the LHC requests CMS to turn off the sensitive detectors to allow a beam dump or beam studies. Given is the luminosity as determined from counting rates measured by the luminosity detectors after offline validation. This preliminary calibration is based on short van-der-Meer scans performed routinely by LHC in every fill.

![Cumulative offline luminosity versus day delivered to (blue), and recorded by CMS (orange) during stable beams and for p-p collisions at 13 TeV centre-of-mass energy in 2015. The delivered luminosity accounts for the luminosity delivered from the start of stable beams until the LHC requests CMS to turn off the sensitive detectors to allow a beam dump or beam studies. Given is the luminosity as determined from counting rates measured by the luminosity detectors after offline validation. This preliminary calibration is based on short van-der-Meer scans performed routinely by LHC in every fill.\label{fig:lumi}](figures/int_lumi_per_day_cumulative_pp_2015.png "Integrated Luminosity")


In November the run was turned over to the Heavy Ion physics program.  There was an initial reference proton collision run at $\sqrt{s}=5$ TeV followed by lead-lead collisions at $\sqrt{s}=5$ TeV/nucleon.  There were 
29.5 pb$^{-1}$ of reference proton collisions delivered, of which 27.9 pb$^{-1}$ were recorded.  Similarly, there were  
0.60 nb$^{-1}$ of lead-lead collisions delivered, of which 0.56 nb$^{-1}$ were recorded. 


On December 15, at the so-called "Data Jamoboree", the CMS experiment presented 33 different physics results from the 13 TeV data.   This is a testament to operational performance of the detector, and the success of 
all the operations tasks that are required to get from data to physics objects with which analysis can be performed. 



## BRIL

The main emphasis of the US-CMS hardware effort as part of the BRIL sub-detector 
group is the pixel luminosity telescope (PLT). 
The PLT provides fast triple-coincidences in 3-layer telescopes, but also at a lower 
rate, full hit-information readout. Based on the latter, detailed studies were performed 
that resulted in a first round of luminosity calculations for the new 13 TeV data set 
by the time of the jamboree. The PLT also participated successfully in the heady-ion 
run at the end of 2015. All BRIL detectors have been shutdown for the year end technical stop.

During the quarter improvements were made to the online monitoring and DQM and these will be further improved during the technical stop to incorporate lessons learned from the running. Also during the shutdown, repairs will be made to two out of 16 telescopes that developed problems during the run. Parts for repair are ready and will be operated in a test stand that can be chilled. The test
stand has been moved to Bldg 168 at CERN and is presently recommissioned.
The PLT is in a transition period and critically dependent on the presence 
of at least one senior researcher at CERN. Currently there is a concern as to how this support will be provided in the future. We might have to re-evaluate the level of support.
This is a critical issue to be watched.
 
In 2015 the pixel luminosity telescope was a very successful addition to the CMS detector
and is expected to provide high-quality luminosity information with continuously improved
certainty. The PLT's predicted longevity corresponds to an integrated luminosity of 500 fb-1; 
the actual lifetime is subject of continuous efficiency monitoring. The PLT is in a transition period and critically dependent on the presence 
of at least one senior researcher at CERN. Currently there is a concern as to how this support will be provided in the future. We might have to re-evaluate the level of support.
This is a critical issue to be watched.
 

Subsystem   Description                   Scheduled   Achieved
----------- --------------------------- ----------- ----------
BRIL        Hardware installed                  Jan        Jan
BRIL        Ready to deliver Lum             March      March 
BRIL        Ready to deliver bkg nums        May         May

  : BRIL Milestones\label{BRILMIlestones}.


Metric                                      Performance
------------------------------------------ ------------
Fraction of telescopes operational         14/16
Efficiency of delivery of lumi histograms  $>$ 99 % 
Uptime of lumi  histogram production       $\sim$ 99 % 

  : BRIL Metrics\label{BRILMetrics}.

## Tracker
The tracker successfully shifted from proton-proton running to
heavy ion running.

The 2015-2016 YETS will be dominated by cooling system maintenance.
We will also explore increasing the readout rate of the SLINK cards in the
pixel DAQ. Work on the power issues in the BPiX will only commence once
the magnet can again be energized.



### Pixels

In the last proton-proton running, pixels accounted for 13\% of the CMS downtime.
Heavy ion data taking began with a modified proton-proton firmware that forced a
large ($>$10\%) deadtime on CMS. The new heavy ion firmware was installed on
December 2 and worked well, reducing deadtime due to pixels well below 1\%.
During the Heavy ion running, pixels accounted for 11\% of the CMS downtime.
98.6\% of the pixel tracker channels are working. 99.8\% of the FPiX channels
are working.

#### Strips
In the last proton-proton running, strips accounted for 8\% of the CMS downtime.
Heavy ion data taking began with a new firmware that compresses the strip
data. This firmware works well at rates below 10 kHz (typical rates were
around 4-5 kHz). During the Heavy ion running, strips accounted for 21\% of
the CMS downtime. 97.3\% of strip tracker channels are working.

\begin{table}[htp]
\caption{Tracker Milestones}
\begin{center}
\begin{tabular}{|l|l|r|r|}
\hline
Subsystem&Description&Scheduled&Achieved\\
\hline
Tracker & Installation and checkout& & Achieved\\
\hline
Tracker & Tracker operate -15C & & Achieved\\
\hline
Tracker & Pixel operate -10C & & Achieved\\
\hline
Tracker& Ready for proton beams& March & March\\
\hline
\end{tabular}
\end{center}
\label{TrackerMilestones}
\end{table}%

\begin{table}[htp]
\caption{Tracker Metrics}
\begin{center}
\begin{tabular}{|l|c|c|}
\hline
 &Pixels&Strips\\
\hline
\% Working channels & 98.6 & 97.3 \\
\hline
Fraction of deadtime attributed& 13\%& 8\%\\
\hline
\end{tabular}
\end{center}
\label{TrackerMetrics}
\end{table}%


## ECAL

The ECAL has performed excellently in the first running period of Run II spanning the
previous quarter. The fraction of active channels is 99.07\% in the barrel, 98.71\% in the endcaps, 
and 96.9\% in the preshower. ECAL has operated with very high data-taking and certification efficiency during the whole 2015, causing no significant loss of recorded luminosity. A new pulse fit reconstruction technique has been deployed to mitigate the effects of out of time pileup and after a
commissioning period has been shown to improve resolution and identification efficiency of
electrons and photons. The stability of the new blue laser monitor
has been excellent in Run II. This has allowed the smooth computation of the laser monitoring corrections, to account for radiation-induced response changes. The laser corrections were also used to correct the energy scale at L1 and HLT both in the endcaps and in the barrel for the first time
to significantly improve trigger performance. An intense programme of work has been carried out in the summer and autumn to re-commission the three energy calibration methods ($\pi^{0}/\eta$, phi-symmetry in minimum bias events, and single electron events using the E/p ratio), to understand the response of each calibration method at 13 TeV with respect to 8 TeV, and to evaluate any effects induced by the new reconstruction method. These were validated using $Z\rightarrow ee$ decays. They provide almost the same resolution as in 2012 for non-showering electrons in the barrel, and slightly worse in the endcaps, due to the smaller number of electron events recorded in 2015.




\begin{table}[htp]
\caption{ECAL Milestones}
\begin{center}
\begin{tabular}{|l|l|r|r|}
\hline
Subsystem&Description&Scheduled&Achieved\\
\hline
ECAL & Finish HV Install& Feb & May\\
\hline
ECAL & Baseline levels zero suppression& March & March \\
\hline
ECAL & Complete install HV calib system & April &May\\
\hline
ECAL & Selective readout& June & July\\
\hline
ECAL & Trigger thresholds & June &July\\
\hline
ECAL & Zero suppression thresholds & June & July \\
\hline
\end{tabular}
\end{center}
\label{ECALMilestones}
\end{table}%

\begin{table}[htp]
\caption{ECAL Metrics}
\begin{center}
\begin{tabular}{|l|r|}
\hline
Metric&Performance\\
\hline
Fraction of channels operational: EB& 99.1\%\\
\hline
Fraction of channels operational: EE& 98.9\% \\
\hline
Fraction of channels operational: ES& 98.4\% \\
\hline
Fraction of deadtime attributed& $<$ 1%\\
\hline
Resolution performance & Equivalent to Run 1\\
\hline
\end{tabular}
\end{center}
\label{ECALMetrics}
\end{table}%





## HCAL

During the final months of 2015, the HCAL project remained focused
on two principal tasks:  the operation of the HCAL detector for LHC collisions at 13 TeV in Run II, 
and the development and preparation for the
installation of the Phase I upgrades.

Since the beginning of Run II, the entire HF detector has been read out with the new $\mu$TCA electronics, 
which also produce trigger primitives that are sent to the Regional Calorimeter Trigger (RCT).  
Operating the HF with new back-end electronics was not without challenges.  
Issues with a loss of synchronization in the HF readout 
and pipeline corruption in the new HF $\mu$HTRs
affected approximately 490~pb$^{-1}$ of data of the 2.8~fb$^{-1}$ collected. 
To accommodate this reduction, special MC datasets that reproduce the effect seen in collision data are being generated.  
Resolving these problems 
resulted in improvements in monitoring procedures and 
alarms, along with significant changes to online data quality monitoring plots, 
which are currently being implemented in preparation for 2016.  
During the final four weeks of 2015 p-p collisions, 
data-taking proceeded smoothly with a loss of less than 1\%\ of the 
collected data due to HCAL problems (10.7~pb$^{-1}$  out of 1184~pb$^{-1}$ ).
This good performance continued in the heavy-ion running.


In Run II, HE signals have continued to degrade 
due to radiation damage of the scintillator tiles and the wavelength-shifting fibers.  
As a result, working groups were formed in November to study the effect in detail 
and prepare for the possibility of a partial replacement of HE megatiles during Long Shutdown 2. 






\begin{table}[htp]
\caption{HCAL Milestones}
\begin{center}
\begin{tabular}{|l|l|r|r|}
\hline
Subsystem&Description&Scheduled&Achieved\\
\hline
HCAL& Fully functional HCAL in CRAFT runs & March & March\\
\hline
HCAL& prepared to do HF Phase scan & &\\
&and $\phi$ symmetry calibration analysis& May& May\\
\hline
HCAL&New HBHE backend operating in& &\\
& parallel with legacy system& July & July\\
\hline
\end{tabular}
\end{center}
\label{HCALMilestones}
\end{table}%


The status of the 2015 HCAL metrics is as follows:
\begin{itemize}

\item Fraction of channels working\\
in HF, 1 channel out of 1728 dead \\
in HBHE, 7 channels out of 5184 dead\\
in HO all 2150 channels work.\\
In total $>$ 99.9\%\ working channels
\item Fraction of downtime attributable to HCAL with magnet on since LS2, 0.5\% 
\item Intercalibration uniformity between individual HCAL towers\\
HB: 1--2\%  \\
HE: 1--2\%  \\
HF: 1--2\% 
\end{itemize}




## EMU

### Data taking and maintenance at Point 5

Data taking in this quarter was smooth with about 6\% of the luminosity losses assigned to the CSC system.

In November, the CSCs began using recuperated CF$_4$.  In 2015 the total volume of recuperated CF$_4$ was 60m$^3$ with a recuperation efficiency of 25\%. This is lower than what expected (~50\%) and will be corrected in 2016. The fraction of argon in the recuperated gas was relatively high (27\%), thus requiring appropriate tuning of the mixer to maintain a stable mixture. 

In December, a water leak in YE+1 caused HV trips in two CSC chambers, and caused CMS to run without the whole ME+1 station for the last few days of the heavy ion run.  After the cooling circuit was turned off, the tripping chambers returned to normal functioning after a short time.  A more through search for the leak and potential damages will be done during the YETS.  Contrary to the initial plans for the YETS,  the CMS heavy elements will need to be opened in order to perform the repairs.


### DPG

CSC spatial resolutions were evaluated with the 2015D data.  They are consistent with earlier measurements from Run 2, and now have a statistical precision of about 0.04\%.  Note that systematic effects such as those from pressure variation are much larger. The analysis confirms the improvement of the spatial resolution of the innermost ME1/1a region ($2.1<|\eta|<2.4$) due to the new electronics and the 3-strips readout un-ganging.  

Many plots and results on muon performance were approved for public release, including segment reconstruction efficiency, timing distributions, and event displays with muons in the CSCs. (https://twiki.cern.ch/twiki/bin/view/CMSPublic/CSCDPGPub151125)



\begin{table}[htp]
\caption{EMU Milestones}
\begin{center}
\begin{tabular}{|l|l|r|r|}
\hline
Subsystem&Description&Scheduled&Achieved\\
EMU& CSC ready for collisions& May & April \\
\hline
EMU& Calibration for HLT and & &\\
& Offline included in DB & July & November\\
\hline
EMU & Fine timing adjustments & & \\
&with collision data completed & July &July \\
\hline
\end{tabular}
\end{center}
\label{EMUMilestones}
\end{table}%



\begin{table}[htp]
\caption{CSC Metrics}
\begin{center}
\begin{tabular}{|l|c|}
\hline
 \% Working channels & 99.0\%  \\
\hline
Fraction of deadtime attributed& 6\%\\
\hline
Median spatial resolution & 129 $\mu$m\\
\hline
\end{tabular}
\end{center}
\label{TrackerMetrics}
\end{table}%

## DAQ

The Main focus of the DAQ efforts during this quarter was on the preparation and then operation for Heavy Ion runs. During Heavy Ion collisions, the planned Level 1 trigger rate was ~10 kHz but due to non-zero suppressed readout, expected events sizes were ~17 MB. To handle this increased payload from the detector, the number of Readout Units (RU) were increased, the first stage of the Event Building (FEDBuilder) data network was expanded with a “FAT tree” architecture that provides full connectivity between all FrontEnd ReadOut Link (FEROL) modules and RU’s. Ten more Builder units were also added to handle the larger event data.  These modifications will also make it easier to handle the 2 MB events expected after the installation of additional Pixel layers and when the LHC reaches higher luminosities with larger pileup in 2016. 

US DAQ groups have led efforts for the EvB performance optimization and network routing and configurations for Heavy Ion conditions. The selection of High Level Triggers was relaxed during the Heavy Ion run with the goal of selecting as many events as the system could handle. In the end this resulted  6-7 k events/s corresponding to 5-6 GB/s aggregate archival rate to the Storage Manager. 
Figure \ref{fig:daq} contains a screen shot from File Based Filter Farm monitoring page from one of the last Heavy Ion Runs. Total HLT output was 5.3 GB/s averaged over one Lumi Section.  While there were a variety of issues that arose while stressing the system in this way, the data was successfully acquired at these rates and many valuable lessons were learned that can apply to future higher luminosity running. 
\begin {figure}[htp]
\begin{center}
\includegraphics  [width=4in] {PastedGraphic-1.pdf}
\caption{DAQ File Based Filter Farm Monitor page showing the merger status for each Lumi Section for one of the last runs of Heavy Ion running period.  Total output from all HLT nodes was ~5.3 GB/s. The figure also lists the large number of different output streams HLT nodes produced.}
\label{fig:daq}
\end{center}
\end{figure}


\begin{table}[h]
\caption{DAQ Milestones}
\begin{center}
\begin{tabular}{|l|l|r|r|}
\hline
Subsystem&Description&Scheduled&Achieved\\
\hline
DAQ& Hardware Installation of DAQ2& & \\
& with new HLT nodes complete& April & April \\
\hline
DAQ& Complete DAQ2 is operational && \\
&for collisions& July &   May\\
\hline
DAQ&$\mu$TCA DAQ link commissioned & & \\
&for new trigger and HCAL FEDs& July &  June \\
\hline
DAQ&DAQ2 with Run I design performance & Sept. &Sept. \\
\hline
\end{tabular}
\end{center}
\label{DAQMilestones}
\end{table}%

\begin{table}[htp]
\caption{NEEDS UPDATING DAQ Metrics}
\begin{center}
\begin{tabular}{|l|c|}
\hline
 Dead time due to trigger throttling& $<$1\%  \\
\hline
Downtime due to DAQ& negligible\\
\hline
\end{tabular}
\end{center}
\label{DAQMetrics}
\end{table}%


## Trigger

During this quarter the US groups continued their work on the regional calorimeter (RCT) and the endcap muon triggers.  

### Regional Calorimeter Trigger

During the previous three months, the CMS RCT has participated in the 25 ns LHC proton-proton collisions and the Pb-Pb Heavy Ion program. During the  entire time, the calorimeter triggered with the RCT and Stage-1 MP7.  Some minor interventions were required, in a couple of instances this required  replacing receiver cards, but others involved reloading firmware on oRMS  by cycling crate power between fills, and one remote firmware reload of  an oRSC, the first time this has been required since the oRSCs were  commissioned.

The entire Stage-2 Layer-1 Calorimeter trigger was added to the RCT DAQ  partition.  This included moving the Layer-1 AMC13's to the RCT TCDS  partition (including returning TTS states), adding 3 FEDS to be readout,  and a new Layer-1 worker cell in the RCT Trigger Supervisor. The worker  cell has limited functionality, mainly responding to CMS CDAQ state  transitions.  The Layer-1 input links can be monitored in real-time  using a script.  

To analyze the new Layer-1 data, an unpacker has  been developed and is in use.   Currently, we are comparing the e/g and sums emulated with Trigger Primitives  (TPs) received by Layer-1 to those read out.  ECAL has excellent agreement  (the source is the same, ECAL TCCs) and there are issues at about the 10\%  level with the HB/HE, thought to be due to incorrect timing. 

### Endcap Muon Trigger

The Rice University, Northeastern, and University of Florida groups have successfully maintained on-call coverage of the CSC Track-Finder during the reporting period, which included the end of proton collision operations in 2015 and the heavy-ion running period. Only a few instances needed intervention such as an MPC replacement after errors were observed in DQM coming from some chambers. A few minor interventions were required on the B904 test stand during this period as well to keep it up-to-date and functional. Additional plots have been added to the CSCTF DQM to aid in identifying any CSCs sending data out of range, which indicates a problem, and when synchronization error flags are sent with CSC trigger primitive data. The plots streamline the identification and diagnosis of the problems for the CSC experts.


\begin{table}[h]
\caption{Trigger Milestones }
\begin{center}
\begin{tabular}{|l|l|r|r|}
\hline
Subsystem&Description&Scheduled&Achieved\\
\hline
TRIG&Legacy RCT ready for physics&June &June\\
\hline
TRIG&MPC ready for physics&June &June\\
\hline
TRIG&CSCTF Ready for physics&June & June\\
\hline
TRIG& Stage-1 Layer-1 calorimeter trigger&&\\
& ready for physics &Sept.& Sept.\\
\hline
\end{tabular}
\end{center}
\label{TriggerMilestones}
\end{table}%

\begin{table}[htp]
\caption{Trigger Metrics}
\begin{center}
\begin{tabular}{|l|c|}
\hline
 Frac legacy RCT channels & 100\%  \\
\hline
Frac of deadtime attributed legacy RCT& 0.6\%\\
\hline
Frac of MPC Channels& 98.3\%\\
\hline
Frac of EMUTF Channels& 100\%\\
\hline
Frac of deadtime attributed to legacy EMUTF& 0\%\\
\hline
Frac of Stage-1 Layer-1 Channels & 100\%\\
\hline
Frac of deadtime attributed to Stage 1 Layer 1 & 0\%\\
\hline
\end{tabular}
\end{center}
\label{TriggerMetrics}
\end{table}%

