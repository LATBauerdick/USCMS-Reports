\clearpage
# Detector Operations

Through this quarter, CMS has been operating smoothly with no
significant issues affecting performance. Since August the data taking
efficiency has consistently been greater than the 92.5% average efficiency of last year.
It is important to note that the new pixel detector is now working
better than the old one and can handle data at higher rates.

![Luminosity, delivered by the LHC and recorded by CMS, through October
2.](figures/int_lumi_per_day_cumulative_pp_2017_Oct_2.png)

The performance of the LHC has been excellent although there have been
some issues. In 2017, with 25ns bunch spacing and sufficient intensity
in the machine, LHC found it difficult to ramp the beams due to losses
originating in 16L2 (this is where a dipole was replaced). These losses
were “UFO-like” but lasted much longer than a classic UFO. Diagnostics
were tuned to identify these losses, and with these new tools they were
able to observe that the losses essentially disappeared when they
switched to an 8b+4e filling scheme. In this scheme, the LHC has 8
filled bunches followed by 4 empty ones. This reduces the e-cloud to
$\sim 30\%$ of its value with the same number of bunches but with 25ns
trains. This scheme reduces maximum number of bunches in the machine to
1916 (1909 colliding) compared to $\sim 2550$, which was the aim for
2017, with BCMS.

In order to gain back the lumi lost by having fewer bunches, LHC is
doing the following

- reduce $\beta* = 40cm \rightarrow 30$cm
- reduce cthe rossing angle (both at beginning of and within the fill)
- increase intensity per bunch (fill 6255 has 1.25e11 p/bunch)

This appears to be working, although it is resulting in higher pileup at
the detector. CMS is working to modify the trigger menu to cope with the
increased pileup, in the meantime, luminosity leveling is being used.

## BRIL

The Pixel Luminosity Telescope (PLT) together with the fast beam
condition monitor (BCM1F) and HF are operational and providing online
luminosity measurements continuously, with PLT as primary luminometer
most of the time. The fast readout used for online luminosity for all
telescopes works but two telescopes exhibit degraded full pixel
information that is used for track-based studies. Reconstructed tracks
are also used for fast measurements of the beam spot and allow tracking
of beam conditions. Corrections for efficiency and accidentals are
obtained from analysis of tracks offline, and with fast turnaround after
completion of each fill now. These corrections change with beam
conditions and expected reduction of efficiency over time. Luminosity
based on full track measurements is prepared for broadcasting in the
online monitor. After the technical stop (TS) in September PLT observed
a few percent higher visible cross section (luminosity). During the TS
the detector with the neighboring pixel detector warmed up contributing
to this change in working point. Also during this quarter several LHC
high-luminosity runs were used to study beam conditions.

The PLT detector is scheduled for replacement in 2019 due to the
expected radiation damage. As the detectors use analog readout old parts
have to be secured. Critical is the availability of port cards and
slow-hub chips for four opto-motherboards – only 4 are left. A design
modification for an alternative chip is explored. A test stand for
component testing is prepared at Rutgers and CERN.

  Working Metric                                Performance
  ------------------------------------------- -------------
  Fraction of telescopes fully operational              90%
  Efficiency of delivery of lumi histograms         $>$ 99%
  Uptime of lumi histogram production               $>$ 99%
  Lumi lost                                           0 /pb

  : BRIL Metrics

\[BRILMetrics\]

  Subsystem   Description                   Scheduled   Achieved
  ----------- --------------------------- ----------- ----------
  BRIL        Update Lumi for 2016            March 1    March 1
  BRIL        Ready for Physics                 May 1      May 1
  BRIL        Improve 2017 Lumi numbers         Dec 1 

  : BRIL Milestones

\[BRILMIlestones\]

## Tracker

Substantial improvements to tracker operations this quarter. Primarily
in Pixels.

### Pixels

The timing setting strategy for the Barrel (BPiX) layer 1 and layer 2
settled in July and we have been operating in this mode. Some of the
issues in BPiX layer 1 performance were due to a mistake in the
assignment of constants for unpacking and reconstruction. The mistake
was noticed in mid-September and the re-reconstruction will be done with
the fix. Another source of broken clusters in the BPiX has been
addressed with a patch: a minimum pulse-height for a registered hit is
forced in the data. Finally, an automatic reset mechanism was added to
recover lost efficiency in the layer 1 chips. The Swiss consortium still
plans to have a new layer 1 chip for a replacement of the layer 1 in
Long Shutdown 2 (LS2). The submission will consist of the layer 1 chip,
a new Token Bit Manager (TBM) chip, and perhaps some auxillary chips for
the Pixel Luminosity Telescope.

An automated recovery for non-responsive TBMs was put into production in
August. Recovery times of 5-10 minutes were reduced to 30s (similar to
Single Event Upset (SEU) recovery in the phase 0 detector). Another
recovery method (10s per) was implemented to discover SEU’s when a Port
Card is non-responsive in the Forward Pixels (FPiX). Both mechanisms are
triggered by the number of problematic modules seen in the detector.
These recovery mechanisms are triggered on the order of 10 times/fill,
and channel status is directly propogated to be available for
reconstruction and triggering purposes. We are now in the position of
fine tuning of the faster recovery mechanisms. Additionally, the time
the detector spends for a fast reset was reduced and the time spent in
configuring and Data Acquisition state changes was reduced.

The first data labeled "good" for the pixel detector occurred on June 16
and the milestone for collisions has been indicated for that date.
Earler data suffered a bit due to non-optimized seetings, but
performance of the phase 1 detector was as good as the phase 0 detector
early on and has been improving as we learn.

### Strips

The main issues in the strips are ongoing maintenance issues, such as
power supply swaps, condition sensors etc. The Operations crew tries to
minimize downtimes as these issue pop-up. The strips are now also
looking to reduce time spent in SEU recovery and are investigating an
issue where the strips DAQ occasionally holds off triggers.

                                         Pixels   Strips
  ------------------------------------- -------- --------
  % Working channels                      95.4     96.2
  Downtime attributed in pb$^{-1}$       202.4    191.6
  Fraction of downtime attributed (%)     13.6     12.3

  : Tracker Metrics

\[TrackerMetrics\]

  Subsystem   Description                          Scheduled   Achieved
  ----------- ---------------------------------- ----------- ----------
  Tracker     Pixel Phase 0 Detector Removed          Feb 15     Jan 23
  Tracker     Pixel Phase 1 Detector Installed        Mar 30     Mar 12
  Tracker     Pixel Phase 1 Detector                         
              Ready for Collisions                     May 5     Jun 16

  : Tracker Milestones

\[TrackerMilestones\]

## ECAL

The ECAL continued to operate smoothly in the third quarter with no
major issues. The low voltage and laser systems required no significant
interventions or maintenance. The high voltage system required the
replacement of one failed module. The firmware for oSLB link boards was
updated to provide more reliable data transfer to the level 1 trigger.
The Detector Control System software was updated to provide more
information to the shifter. Refined laser calibrations were applied to
both the barrel and endcap to enhance the trigger turn on performance.
Updates in the pedestals improved stability and the calibration is
improving progressively as more 2017 data is accumulated.

  Metric                                   Performance
  -------------------------------------- -------------
  Fraction of channels operational: EB             99%
  Fraction of channels operational: EE           98.4%
  Fraction of channels operational: ES           99.9%
  Downtime attributed pb$^{-1}$                     91
  Fraction of downtime attributed                   7%
  Resolution performance                          2.5%

  : ECAL Metrics

\[ECALMetrics\]

  Subsystem   Description                      Scheduled   Achieved
  ----------- ------------------------------ ----------- ----------
  ECAL        Refurbish Maraton to provide               
              redundant thermal interlock        March 1     March1
  ECAL        Replace Laser Diode                March 1    March 1
  ECAL        Ready for Beam                       May 1      May 1
  ECAL        Preliminary Calibration            June 15    July 15

  : ECAL Milestones

\[ECALMilestones\]

## HCAL

During this quarter, the HCAL Operations group focused on continuing to
commission the HF and partial HE Phase I upgrades, and on taking data.

For HF, the upgrade consists of installing dual-anode readout and
installing new ftont-end electronics to support increased number of
channels. The old QIE8s (7bit ADC) were replaced with QIE10s (8bit ADC).
The new front-end electronics also has an embedded TDC which will be
used to discriminate physics signals from showers in the HF calorimeter
from spurious signals due to Cerenkov light from particles directly
hitting the photo-tube windows.

All the components for the HF upgrades were installed ahead of schedule
by mid-March, and the detector was ready for physics by May 1. The low
voltage supply trips, which were caused by single-event-upsets, were
resolved by replacing the offending supplies and, in the process,
reducing the number of supplies by a factor of two. Only 7 pb$^{-1}$ of
data have been lost due to low voltage trips in the two months since the
supplies were replaced (less than 0.1%). All the new handles to achieve
noise reduction are in place and a substantial reduction in missing Et
trigger rates has been achieved. Energy recovery using the “other” anode
when one anode has an out-of-time signal has been successfully
implemented.

For HE, one upgraded HE readout box (out of 36) was installed to obtain
experience with upgraded system. This readout box (HEP17) has silicon
photomultipliers (SiPMs) instead of HPDs and has the new version of the
QIE frontend chip. (QIE11)The upgraded HE readout box (HEP17) was
commissioned rapidly, has been calibrated and has performed well
throughout the run.

Planning to install the complete HE upgrade in 2017-18 YETS is nearly
complete. The HCAL Operations Project Manager recommended installation
of the full HE Upgrade during the 2017-18 YETS to the CMS MB on
September 4 and to the LHCC on September 12. The feedback from both the
MC and LHCC was positive and supportive.

The Installation Readiness Review for the HE Phase-1 upgrade during YETS
17/18 has been scheduled for November 6.

[*Metrics and Milestones*]{}\
June 1 Milestones\
HF Detector Commissioned. This milestone was achieved May 1.\
HCAL Ready for Physics. This milestone was achieved May 15.\
July 15 Milestones\
Operate HCAL efficiently with less than 1% CMS data lost due to HCAL.
Achieved July 10.\
HCAL Calibration at the 1% to 2% level. Currently 3% .

  Metric                                   Performance
  -------------------------------------- -------------
  Fraction of channels operational: HF           100% 
  Fraction of channels operational: HE         99.85% 
  Fraction of channels operational: HB          99.77%
  Fraction of channels operational: HO          99.72%
  Downtime attributed pb$^{-1}$                   77.2
  Fraction of downtime attributed                0.3% 
  Abs Energy Calibration                            3%
  Inter-calibration Uniformity                      3%

  : HCAL Metrics

\[HCALMetrics\]

  Subsystem   Description                  Scheduled   Achieved
  ----------- -------------------------- ----------- ----------
  HCAL        HF Phase 1 Installed           April 1   March 15
  HCAL        HF Detector Commissioned        June 1      May 1
  HCAL        Ready for Physics               June 1     May 15
  HCAL        Data Loss $<  1\%\ $           July 15    July 10
  HCAL        1% to 2%  Calibration          July 15 

  : HCAL Milestones

\[HCALMilestones\]

## EMU


As described in the previous quarterly report, the CSC system was
operating with a total of four of the 540 chambers disabled: two
chambers (ME-1/1/34 and ME-1/1/35) without cooling due to water leaks,
and two other chambers (ME-2/1/3 and ME-4/2/21) that are disabled due to
low voltage problems. Both have shown similar problem before, but the
low voltage connections on these chambers are difficult to access. All
four CSCs will be investigated further during the year-end technical
stop 2017-18. To mitigate the effect of the disabled chambers,
particularly those in station 1, new firmware was introduced to the EMTF
trigger to allow some L1 triggers to be generated without requiring a
trigger primitive in layer 1. This increased the efficiency with a very
minor increase in the trigger rate.

A few isolated issues led to most of the downtime attributed to the CSC
for this period. On August 6th, an HV trip was followed by cycling of
low voltage on one half of the CSC system. This required some down time
for recovering the system. On September 8th, an issue with one FED
caused it to be disabled from the run while attempting to debug at the
FED crate. Eventually, an L1A error from ME1/1/2 was found to be the
source of the issue and the optical fiber for this chamber was
disconnected to restore FED functionality. Another incident occurred on
September 10th when one LV Maraton supply tripped causing 9 CSCs from
VME crate +4/5 to go off. Investigation is ongoing. Moreover, a faulty
CANbus line on some of the LV supplies caused the DCS system to issue
erroneous error flags affecting data certification. The supply that was
causing the errors was identified and replaced in Technical Stop 2. The
erroneous marking of data was corrected in the data certification data
base.

Additional minor issues with CSC data taking were detected and fixed in
this period. A longstanding issue with one chamber not sending data to
the trigger system was found to be due to a timing error in an adjacent
chamber and subsequently fixed. A strange spatial distribution of muons
in cosmic ray runs was tracked down to an automatic synchronization
feature that has been disabled in the cosmic trigger configuration; the
normal distributions were restored when this was re-enabled. It was also
discovered that the EMTF trigger occasionally fails to get the beam
crossing synchronization signal (BC0) from some CSCs. A change in the
EMTF firmware mitigated the effects of this, but the cause is still
under study.

A tag-and-probe analysis of 2017B $Z\to\mu\mu$ data samples shows very
high and uniform segment and trigger primitive (LCT) efficiencies in the
CSC stations. The offline timing calibration has been completed. After
aligning the cathode strip timing the anode wire times of all CSC have
been re-aligned after a $\sim2$ ns shift of the LHC clock phase had been
introduced early in the year. Segment and muon times are properly peaked
at zero and their widths are comparable to 2016 data.

The HV settings are under study, in anticipation of find an operating
point with a lower gain. A better understanding is required of the
interdependencies of gain vs. efficiency before the gains are
systematically lowered.

Both chambers (ME1/1 and ME2/1) that were being irradiated at GIF++
since January 2016 have now reached their targets goals of 330 mC/cm and
540 mC/cm of accumulated charge, respectively. These values represent
three times the charge expected after 3000 fb$^{-1}$ of integrated
luminosity at the HL-LHC. The chambers have not shown signs of aging and
are perfectly functioning. They have been switched to a gas mixture
which uses only 2% CF4 (instead of 10%), in order to investigate a
mixture that meets CERN strategy to fulfill new EU regulations for
greenhouse gas emission.

In the management news, Armando Lanaro (U. Wisconsin) was reappointed
for an additional two years term as subsystem leader for the CSC. The
new deputy manager Manuel Franco Sevilla (UCSB).

  % Working channels                   98.5%
  --------------------------------- ------------
  Downtime attributed pb$^{-1}$         50.5
  Fraction of downtime attributed        5%
  Median spatial resolution          126 $\mu$m

  : CSC Metrics

\[CSCMetrics\]

  Subsystem   Description                          Scheduled Achieved
  ----------- ---------------------------------- ----------- -----------------------------
  EMU                                                  May 1 April 29
  EMU                                                 July 1 January 29
  EMU         New HV settings for reduced gain      August 1 Reschedule to December 2017

  : EMU Milestones

\[EMUMilestones\]

## DAQ

The DAQ system has been working without major problems. The EOS problems
which plagued tier0 for a few weeks in summer did not cause any downtime
thanks to mitigating actions taken by the SMTS team. The DAQ group
continues to maximize the data-taking efficiency, in particular related
to the new pixel detector. Run control has been improved to shorten the
time before good physics data can be recorded following the ramp-up of
the high voltage of the pixel and strip trackers. Several control
sequences were added in the TCDS system to minimize the downtime
required to reset pixel readout components. Software and firmware have
been continuously improved to provide better monitoring and error
reporting. The DAQ expert system becomes more powerful in advising the
shifter to take the best recovery action in case of problems. The LHC
technical stops and machine development periods were used to consolidate
the system configuration and robustness. This mitigates the risk of
operator mistakes and reduces the workload on the sysadmin team.

The new slink-express sender cards developed for the ECAL readout have
been delivered on time. However, due to firmware problems on the ECAL
side, the installation was postponed to the YETS.

The HLT online cloud is now routinely used for offline event processing
between LHC fills. The quota of HLT CPUs allocated to the cloud has been
gradually increased and has reached 66%, which corresponds to last
year’s level. The online cloud is number three in the list of number of
processed jobs (behind FNAL and T0). We plan to run the cloud on a large
fraction of the HLT nodes during most of 2019, the first year of LS2.
This corresponds to about 550kHS06.

“The Phase-2 Upgrade of the CMS DAQ Interim Technical Design Report” was
submitted to LHCC on time. It documents the requirements on the phase-2
DAQ system as collected from other subsystem TDRs. A baseline system
design has been developed, required R&D work is described, a timeline
has been worked out, and milestones leading up to the TDR in 2021 were
set. In addition, future R&D directions have been identified which would
provide a more versatile and cost-effective system.

The development on the new online-monitoring system (OMS) is
progressing. All database tables and procedures used by the current WbM
were documented. This is a tedious, but important step in restructuring
the data aggregation layer. The next step is to understand how to design
the new database structure for the OMS system. In parallel, the design
and implementation of the meta-data catalogue is ongoing. This catalogue
will be the core of the new OMS system. It will be used to map public
accessible data to the underlying database tables. A review of the new
system is planned for the CMS week in December, where the plan for 2018
shall be developed.

  Dead time due to backpressure      0.5%
  --------------------------------- ------
  Downtime attributed pb$^{-1}$      2.8
  Fraction of downtime attributed    2.1%

  : DAQ Metrics

\[DAQMetrics\]

  Subsystem   Description                        Scheduled   Achieved
  ----------- -------------------------------- ----------- ----------
  DAQ         New sub-systems integrated             Apr 1     Jun 15
  DAQ         Event builder expanded,                      
              re-optimized for larger events         Jun 1      Apr 1
  DAQ         Old HLT Nodes replaced                       
              and new nodes commissioned             Jun 1     Jun 21
  DAQ         Prototype of OMS (new WBM)                   
              ready for field tests                 Dec 31 

  : DAQ Milestones

\[DAQMilestones\]

## Trigger

During this quarter the US groups continued their work on the Layer-1
calorimeter (CaloL1) trigger upgrade, and the endcap muon trigger
upgrade systems as both continued reliable data-taking for this year.

### Endcap Muon Trigger

The Northeastern, Rice University, and University of Florida groups have
maintained the EMTF system 24/7 during operations this quarter. An MPC
firmware update deployed system-wide has improved robustness of the
optical link communication. Additionally, the firmware of the EMTF was
revised to more robustly recover the BC0 orbit signal. In an effort to
improve the reliability of the online control PC, which on rare occasion
has required a reboot because of its PCIe controller, we installed a new
PCIe driver into the kernel of the control computer and updated the
firmware as well as the online control software accordingly. In order to
recover inefficiency from two ME1/1 CSC chambers that had been turned
off due to a water leak, new firmware was deployed that had looser
tracking requirements in the high rapidity region. The corresponding
increase in the single muon trigger rate was about 1%. New DQM plots
have been deployed online to monitor the RPC detector inputs to EMTF. A
study of the performance of EMTF with the addition of RPC hits in 2017
running shows that we have increased the efficiency back to the level of
the legacy trigger (so no reduction in efficiency), and increased the
rate reduction further from a factor 2 to a factor 3 for a nominal 25
GeV threshold.

### Layer-1 Calorimeter Trigger

The Layer-1 Calorimeter Trigger (CaloL1), built by the University of
Wisconsin - Madison, is a part of the complete Calorimeter Phase-1
Trigger Upgrade. CaloL1 was in continuous operation during the LHC
proton physics run in the third quarter of 2017.

In late July two updates were made to CaloL1 Look-Up Tables (LUTs).
Software and firmware was updated to add a second set of calibration
LUTs, previously hardcoded in firmware. At the same time there was an
update to properly account for HCAL tower saturation.

A number of link issues occurred during this period. HCAL had a problem
with low optical power from one link to CaloL1. The uHTR was replaced. A
couple of ECAL links also developed optical power issues, and this was
resolved by simply moving the fiber on the ECAL side to the RCT position
in the dual-transmitter VTTx.

ECAL oSLB firmware was updated to help mitigate the errors due to the
increased electrical activity on the ECAL Barrel Trigger Concentrator
Cards (TCCs). This caused more corrupted data sent to CaloL1, appearing
as CRC and BC0 errors on our ECAL links. There were some issues with the
TCC configuration sequence in the ECAL software at first, but they were
fixed without any loss of physics data.

Another update was done to the infrastructure. The Ethernet-RS232 module
used with the Rack Monitor Cards was becoming difficult to maintain due
to requiring a driver on the DCS PCs. This also prevented having a truly
redundant system. This module was replaced with a Moxa NPort Express
DE-311 which operates like a TCP server. The DCS scripts were updated
and monitoring scripts updated to use TCP.

  Frac of MPC Channels
  -------------------------------------------------- ------
  Frac of Upgrade EMUTF Channels                      100%
  Deadtime attributed to EMTF pb$^{-1}$               47.5
  Fraction of deadtime attributed to EMTF             3.3%
  Frac of Calo. Layer-1 Channels                      100%
  Deadtime attributed to Calo. Layer-1 pb$^{-1}$       0
  Fraction of deadtime attributed to Calo. Layer-1     0%

  : Trigger Metrics

\[TriggerMetrics\]

  Subsystem   Description                         Scheduled   Achieved
  ----------- --------------------------------- ----------- ----------
  TRIG        EMTF commissioned with                        
              endcap RPC input                      April 1   April 27
  TRIG        EMTF ready for Physics                  May 1     May 29
  TRIG        Calo. Layer-1 commissioned                    
              with new ECAL/HCAL/HF Calib           April 1     May 19
  TRIG        Calo. Layer-1 Ready for physics       April 1     May 19

  : Trigger Milestones

\[TriggerMilestones\]
