\clearpage

# Program Manager's Summary

During this quarter, the first quarter of  **calendar year 2016** (2016Q1), 

T.B.R.



Technical Stop
▪ Currently in Year End Technical Stop (YETS) 
▪ Time for maintenance 
▪ Two major items
▪ Magnet cooling system repairs
Valve repaired
Cold box cleaned
▪ Locating and fixing leak that appeared towards the end of the run

Cold box cleaning
▪ Recall
▪ System flushed with special cleaning fluid ▪ “100’s of grams” of Breox oil were removed 
▪ After cleaning, amount of Breox remaining was below measurable limit 
▪ System is now being “closed up” ▪ Removing shunts and taps
▪ Changing seals and O-rings 
▪ Close up is on schedule

Water Leak
▪ Actions related to December water leak ▪ Recovery:
✓ Three ME1/1 chambers that had been removed were reinstalled after checks/repairs (one with leak, two in path of leak). They operate fine now.
▪ Short term preventive actions:
 
 Reassess all circuits leak tightness at 12 bar
Accessible shut-off valves installed on a fraction of the ME1/1 cooling loops (those inaccessible when CMS is closed)
Humidity and temperature sensors installed inside YE1 nose
Water collectors under YE1 nose and inside YB0
Improved monitoring and stability of endcap circuit pressure
 ▪ Long term
๏
 Consider repair or replace of all ME1/1 cooling circuits in coming shutdowns (EYETS 16/17) – await for recommendation of CMS Technical Incident Panel



The YETS gave opportunity to 



LHC beam commissioning to start one week later than planned
 ▪ Several issues discussed at LMC on 17 & 24 Feb...
▪ Dimple in ATLAS beampipe bellows
▪ Oil leak in LHC transformer
▪ Re-alignment of LHC dipoles in long straight sections near ATLAS and CMS
▪ Decision 24 Feb: delay start of beams one week ▪ Original date of first beams: 21 March
▪ New date of first beams: 28 March



During this quarter LHC and CMS were in a year end technical stop (YETS).  This provided an opportunity to make repairs and enhancements to the various elements of the detector as described below.   Importantly extensive work was undertaken on the CMS magnet's liquid helium system, which include repairs, enhancements and most importantly cleaning.   The cleaning procedure confirmed that the system had been contaminated with BREOX oil which has now been successfully removed.   By the end of quarter the work was completed and process to cool down the magnet and turn on the field was underway.  It is anticipated that the magnet will be at full field by late April.  

During the technical stop there was a campaign of global running with cosmic rays that allowed the commissioning of the detector to proceed without beam.  As there was no colliding beams, we are not reporting metrics this quarter.


In general, Software and Computing was focused on preparations for 2016 data-taking and the high-profile ICHEP conference where results from the 2016 data will be presented.  The Tier-1 and Tier-2 facilities all met their performance metrics, while also undergoing upgrades and enhancements that will improve their throughput.  The Tier-1 facility in particular was engaged in a significant upgrade of the storage systems.  The operations teams wrapped the processing of 2015 data events and then proceeded to deliver simulated events at huge rates, resulting in high utilization of the U.S CMS facilities.  A particular highlight was the use of the Amazon cloud computing service to deliver large simulation samples in time for use in results presented at winter conferences.  The infrastructure and services area made steady improvements to many systems to be prepared for this year’s event rates and complexity.  Quite a few developments across all areas of the program were related to the deployment of multi-core jobs running multi-threaded applications.  A major version of experiment software (CMSSW) was released, which is targeted at 2016 data taking and supports multi-threading and many other features.  The R&D area continues activities that have made immediate impacts on operations and also prepare us for the longer term.

HEPcloud

CMS, OSG, and Fermilab Scientific Computing Division are in process
of transforming part of the U.S. facilities to a services platform
(HEPCloud)
▪ Fermilabprovidesthebackbonedataandsoftwareservices ▪ Allowsaccesstoadiversesetofcomputingresources
▪ EnablesCMSaccessto
★ Commercial cloud computing (amazon.com)
★ DOE/NSF High-Performance Computing centers
▪ CMS gains access to on demand computing resources for fast- turnaround — a potential game changer
▪ N.B.: there is no “direct” cost advantage of going clouds or HPC
▪ actually,~50%moreexpensive—butprovidesnewcapabilities,andopportunities
for cost optimizations: on-demand vs baseline of statically provisioned resources
▪ Strategically important for U.S. and FNAL —
— Great example of U.S. leadership in HEP computing!

First large-scale Fermilab HEPcloud tests
▪ CMSgrantfromAmazonWeb Services
★ i.e. ~ free resources for the test ▪ Successful tests
▪ Diversity: All CMS production workflows
▪ Scale:Metgoalof60,000 simultaneous jobs
▪ Physics:518Meventsgenerated ▪ Used for results shown in
March conferences.

### Common Ops items

 Analysis Support and LHC Physics Center
▪ analysis support to U.S. CMS scientists through LPC and S&C support teams
★ 2 FTE for Analysis, Computing, Event Support, provided through LPC organization, supporting physicists across whole U.S. CMS, not just at LPC
★ enables a very active program of training, workshops, tutorials, fora, schools ▪ support for a Guest and Visitors program
★
★
– –
facilitates CMS members to spend time at the LPC and Fermilab, to work on projects for hardware, software, typically combined with their physics interests
2016 program supports 23 individuals, list was just decided by G&V committee junior(14)andsenior(9)G&VbothfromU.S.(14)andinternational(9)collaboratinginstitutes projectsspanupgradeR&D,simulation,softwaresupportforupgrades,commissioning,etc
▪ Run Coordination support: “Web Based Monitoring” Tools ▪ Run Coordination and DAQ ran CMS-wide review
★ Recommendation that the WBM project be reorganized with a clear scope, well defined boundaries, and factorized responsibilities — good progress:
★ New groups joining the effort: DESY, Vilnius, RiceU. → institutional responsibilities
▪ CT-PPS and Fast Timing R&D
▪ receive smallish amounts of support this year



### Overview of 2016 Spending Plan
▪ We all feel the budget squeeze:
▪ DOEfundinglevelverylow,HL-LHCneedshigh
▪ plan for significantly lower spending w/r to 2015 for S&C and Common Ops ( DetOps slightly up )
▪ High level break down:
▪ totalspendingof$36.5M(2015:$38.2M)
▪ Common Operations, $8,682K (2015: $9,317K) ★ of which M&O-A payments $4,333K
▪ Detector Operations, $7,614K
▪ Software and Computing, $15,115K
(2015: 17,483K)
▪ Preparation and R&D for HL-LHC
$4,037K (2015: $4,015K)
  Phase2 R&D 11%
Reserves 3%
Detector Ops 21%
Common Ops 12%
      M&O 12%
-
A
   ▪ Reserve
★ $700K Management Reserve for ops
★ and $400K reserve for HL-LHC 41%
▪ Income:
▪ DOE $25,920K, NSF $9,000k, (2015 was $25,000k and $9,000k) plus a budget carry over from FY15 of $4,411K


### Comclusions

Funding levels in 2015 and 2016 are unprecedentedly low
but possibly there might be silver lining on the horizon
required very significant cuts in S&C engineering and hardware etc
US CMS Ops Program is following a tight spending plan, 
tracking variances and staying within the envelope
In spite of the funding decline, the scope of activities requiring support now includes commissioning of Phase 1 upgrades and R&D for the HL-LHC detector upgrades, in addition to ongoing operations and possible detector repairs
required prioritizing, cut backs and extraordinary resourcefulness
Prioritization decisions are helped by Resource Allocation Advisory Board, which we’ll call into action again soon for the 2017 spending planning

Last year’s data show enticing promises for possible new physics around the corner — let’s focus on getting the best and most out of our beautiful detector!


2016 has discovery potential! Expecting to take 30/fb of pp data
with 150 days of pp physics running (that’s a lot!) + H.I. and special runs

Prep for Phase1 integration, tight 2016/17 shut down work plan
everything has to come together to install Pixels, HCAL, while we’re already transitioning the Trigger

Important for CMS to carefully consider Risks and Benefits of installing detector upgrades in the 2017 extended technical stop!
impact of rad damage to HCAL: we’re getting better input on damage estimates
impact of high pile-up/occupancies on Pixel readout
CERN and LHC is not gonna “wait” for CMS, c/f T’s talk on Monday…
Weigh and balance detector performance impacts vs the risks of commissioning during a highly-intense physics discovery run
We’d better be taking highest-quality data immediately after coming out of the EYETS in 2017, which now happens in the middle of the discovery run, at the highest lumis yet, with Atlas not having to do any major upgrade work!

