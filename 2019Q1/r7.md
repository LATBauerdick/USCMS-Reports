\clearpage
# Summary of Software and Computing

The first quarter of calendar year 2019 marks the start of the Long
Shutdown 2 (LS2) of the LHC, which will last until 2021. But there is no
break for CMS Software and Computing, which continues to operate as
during LHC running periods minus the prompt processing and archiving of
newly recorded data. The U.S. CMS facilities deployed sufficient
computing resources to fulfill the 2019 pledges to CMS, and operational
reliability and availability has been excellent, especially for the FNAL
Tier-1 facility. This has enabled U.S. CMS analysis at very high scales,
contributing significantly to the physics results shown at various
conferences. It also enabled CMS to produce significant numbers of Monte
Carlo events in various running period and detector configurations. This
success was also enabled by using HPC resources like the DOE NERSC
facility and the NSF PSC facility. New allocations for 2019 were granted
through the DOE ERCAP and the NSF XSEDE processes, which we intend to
efficiently apply to solve CMS physics questions. On the infrastructure
software side, the Rucio transition made significant progress and we
presented an upgraded transition plan. We also migrated the remaining
CMS-supported services relying on CMS topology and other metadata from
the CMS-specific (SiteDB) to a WLCG-supported solution (CRIC). Software
development continued with progress in supporting accelerators like GPUs
from the framework and the transition to a community solution for the
geometry description (DD4HEP). The development of tracking on
accelerators like GPUs has made a lot of progress, with the intention to
prepare CMS to run efficiently on the upcoming HPC machines of DOE and
NSF.

[Major milestones achieved this quarter]{.c15} {#h.ihmbw5s3iqa3 .c8}
----------------------------------------------

[]{.c3}

[]{#t.7998e3d60cc4e2866f56bc4935e5c684416e7ee5}[]{#t.0}

+-----------------------------------+-----------------------------------+
| [Date]{.c3}                       | [Milestone]{.c3}                  |
+-----------------------------------+-----------------------------------+
| [January 22]{.c3}                 | [FNAL LPC EOS upgraded to latest  |
|                                   | version and HTCondor pool         |
|                                   | re-factored to container based    |
|                                   | architecture]{.c3}                |
+-----------------------------------+-----------------------------------+
| [January 25]{.c3}                 | [Release of software for MTD TDR  |
|                                   | studies]{.c3}                     |
+-----------------------------------+-----------------------------------+
| [February 5]{.c3}                 | [SciTokens issuer released to     |
|                                   | production CMSWeb instance]{.c3}  |
+-----------------------------------+-----------------------------------+
| [March 9]{.c3}                    | [Release of first version of      |
|                                   | software for low-level            |
|                                   | reconstruction and calibration    |
|                                   | for ultra-legacy processing of    |
|                                   | Run 2 data]{.c3}                  |
+-----------------------------------+-----------------------------------+
| [March 26]{.c3}                   | [CMS onboarded to HEPCloud]{.c3}  |
+-----------------------------------+-----------------------------------+
| [March 31]{.c3}                   | [FNAL Tier 1 site has deployed    |
|                                   | sufficient processing, storage    |
|                                   | and archival resources to meet    |
|                                   | the 2019 CMS resource             |
|                                   | pledge.]{.c3}                     |
+-----------------------------------+-----------------------------------+
| [March 31]{.c3}                   | [All Tier-2 sites have deployed   |
|                                   | sufficient processing and storage |
|                                   | resources to meet CMS resource    |
|                                   | pledges.]{.c3}                    |
+-----------------------------------+-----------------------------------+
| [March 31]{.c3}                   | [Updated Rucio transition plans   |
|                                   | presented at CMS O&C Week]{.c3}   |
+-----------------------------------+-----------------------------------+
| []{.c3}                           | []{.c3}                           |
+-----------------------------------+-----------------------------------+
| []{.c3}                           | []{.c3}                           |
+-----------------------------------+-----------------------------------+
| []{.c3}                           | []{.c3}                           |
+-----------------------------------+-----------------------------------+
| []{.c3}                           | []{.c3}                           |
+-----------------------------------+-----------------------------------+

[]{.c3}

[Fermilab Facilities]{.c15} {#h.7bsbcyazyp2x .c8}
---------------------------

[Q1 of 2019 was the first quarter of LHC Long Shutdown 2.  Throughout
this quarter the Fermilab Facilities continued to provide reliable
custodial storage, processing and analysis resources to U.S. CMS
collaborators.  The site was well utilized, with the facility providing
43.8 million wall-clock hours of processing to CMS.  ]{.c3}

[]{.c3}

[![](images/image2.png)]{style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 258.13px;"}

[Figure 1 shows the site readiness metrics for the quarter. During this
quarter the Tier 1 facility passed CMS site availability metrics 100% of
the time.  Two incidents in this period were caught quickly enough to
not impact delivery of computing resources to CMS:  In preparation for
turning on IPv4/IPv6 on our storage resources the Tier 1 disk dCache had
an IPv6 address earlier than needed, causing transfer failures to some
sites in March.  Also during March the LPC EOS instance was flooded by
user requests, causing it to be unavailable for several hours.  ]{.c3}

[]{.c3}

[In January the FNAL LPC took a day downtime to successfully upgrade EOS
to the latest version, also used by CERN.  The HTCondor batch resources
used by the LPC users were also migrated from a static node to a more
dynamic docker container based architecture, in time for users'
preparations for winter physics conference approvals.  ]{.c3}

[University Facilities]{.c15} {#h.466xxpdryvyl .c8}
-----------------------------

[]{.c3}

[As seen in Figure 2, CMS production and analysis activities this
quarter continued to run at high levels relative to full capacity. Total
analysis usage has remained high and the faction of analysis activity by
U.S. researchers at our sites has returned to the historical average of
\~70% after a lull in activity over the winter.]{.c3}

[]{.c3}

[![](images/image3.png)]{style="overflow: hidden; display: inline-block; margin: 0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 285.33px;"}

[]{.c3}

[Figure 2:  Processing usage at the U.S. CMS Tier-2 sites by month,
broken down by type of usage (production, analysis by U.S. researchers,
and non-U.S. researchers, e.g.) compared to purchased processing
capacity.]{.c3}

[]{.c3}

All of the U.S. CMS Tier-2 sites operated successfully last quarter. On
our two official performance metrics based on CMS test jobs, all sites
were at least 95%
"[[available](https://www.google.com/url?q=http://wlcg-sam-cms.cern.ch/templates/ember/%23/historicalsmry/heatMap?end_time%3D2019%252F04%252F01%252000%253A00%26granularity%3DDaily%26profile%3DCMS_CRITICAL%26site%3DT2_US_Caltech%252CT2_US_Florida%252CT2_US_MIT%252CT2_US_Nebraska%252CT2_US_Purdue%252CT2_US_UCSD%252CT2_US_Wisconsin%26start_time%3D2019%252F01%252F01%252000%253A00%26time%3Dmanual%26type%3DAvailability%2520Ranking%2520Plot&sa=D&ust=1556127443808000){.c6}]{.c5}"
and 83%
"[[ready](https://www.google.com/url?q=https://dashb-ssb.cern.ch/dashboard/request.py/sitereadinessrank?columnid%3D234%23time%3Dcustom%26start_date%3D2019-01-01%26end_date%3D2019-04-01%26sites%3Dmultiple%26timebins%3Dfalse%26nodata%3Dfalse%26binsselect%3Ddefault%26clouds%3Dall%26site%3DT2_US_Caltech,T2_US_Florida,T2_US_MIT,T2_US_Nebraska,T2_US_Purdue,T2_US_UCSD,T2_US_Wisconsin&sa=D&ust=1556127443808000){.c6}]{.c5}[",
which is above the CMS requirement for each of these metrics of 80%. The
U.S. CMS performance goal is 90%, which all sites met except for Caltech
and Purdue. Nonetheless our commitments to CMS were met with success.
]{.c3}

[]{.c3}

The U.S. CMS Tier-2 centers delivered
[[49.4%](https://www.google.com/url?q=http://dashb-cms-jobsmry.cern.ch/dashboard/request.py/consumptions_individual?sites%3DT2_AT_Vienna%26sites%3DT2_BE_IIHE%26sites%3DT2_BE_UCL%26sites%3DT2_BR_SPRACE%26sites%3DT2_BR_UERJ%26sites%3DT2_CH_CSCS%26sites%3DT2_CN_Beijing%26sites%3DT2_DE_DESY%26sites%3DT2_DE_DESY_Test%26sites%3DT2_DE_RWTH%26sites%3DT2_EE_Estonia%26sites%3DT2_EE_Estonia_Test%26sites%3DT2_ES_CIEMAT%26sites%3DT2_ES_IFCA%26sites%3DT2_FI_HIP%26sites%3DT2_FI_HIP_Test%26sites%3DT2_FR_CCIN2P3%26sites%3DT2_FR_GRIF_IRFU%26sites%3DT2_FR_GRIF_LLR%26sites%3DT2_FR_IPHC%26sites%3DT2_GR_Ioannina%26sites%3DT2_HU_Budapest%26sites%3DT2_IN_TIFR%26sites%3DT2_IT_Bari%26sites%3DT2_IT_Legnaro%26sites%3DT2_IT_LegnaroTest%26sites%3DT2_IT_Pisa%26sites%3DT2_IT_Rome%26sites%3DT2_KR_KNU%26sites%3DT2_MY_UPM_BIRUNI%26sites%3DT2_PK_NCP%26sites%3DT2_PL_Swierk%26sites%3DT2_PL_Warsaw%26sites%3DT2_PT_NCG_Lisbon%26sites%3DT2_RU_IHEP%26sites%3DT2_RU_INR%26sites%3DT2_RU_ITEP%26sites%3DT2_RU_JINR%26sites%3DT2_RU_PNPI%26sites%3DT2_RU_RRC_KI%26sites%3DT2_RU_SINP%26sites%3DT2_TH_CUNSTDA%26sites%3DT2_TR_METU%26sites%3DT2_UA_KIPT%26sites%3DT2_UK_London_Brunel%26sites%3DT2_UK_London_BrunelTest%26sites%3DT2_UK_London_IC%26sites%3DT2_UK_SGrid_Bristol%26sites%3DT2_UK_SGrid_RALPP%26sites%3DT2_US_Caltech%26sites%3DT2_US_Florida%26sites%3DT2_US_MIT%26sites%3DT2_US_Nebraska%26sites%3DT2_US_Purdue%26sites%3DT2_US_UCSD%26sites%3DT2_US_Vanderbilt%26sites%3DT2_US_Wisconsin%26sitesSort%3D2%26start%3D2019-01-01%26end%3D2019-04-01%26timeRange%3Ddaily%26granularity%3DMonthly%26generic%3D0%26sortBy%3D0%26series%3DAll%26type%3Dewa&sa=D&ust=1556127443810000){.c6}]{.c5}[ of
all computing time by Tier-2 sites in CMS last quarter, down about 1.4%
from the previous quarter but consistent with historical averages. While
we deliver \~50% of the Tier-2 computing power world-wide, the \~70%
analysis usage by U.S. researchers shows that the extra capacity
delivered above our headcount enables U.S. physics research. The storage
and processing deployment milestones to meet the April 1, 2019 pledge
amounts to CMS were made with success.]{.c3}

[]{.c3}

[The U.S. CMS Tier-3 support team provided help to fifteen sites on a
number of issues mainly related to OSG software upgrades, Singularity,
PhEDEx transfers, XRootD, and HDFS.  We are also preparing for the
transition from OSG CA host certificates to InCommon IGTF or alternative
CA options.  InCommon certificates have been tested and the Let's
Encrypt CA is being explored.  CMS Connect support continued and several
interventions were required to handle issues with problematic users
causing service degradation.  Access to GPU resources at Vanderbilt were
enabled and a workaround to allow interactive access to GPU resources
was documented.  Finally, the support team initiated a significant
effort to verify the functional status of all U.S. Tier-3 sites and to
remove from databases any sites that are no longer maintained.]{.c3}

[Computing Operations]{.c15} {#h.okdiq8v3dzp4 .c8}
----------------------------

[The LHC is in long shutdown until spring 2021. CMS computing operations
is continuing the processing of Run 2 data and starting Run 3
preparations.]{.c3}

[]{.c3}

[Distribution, archival, and reconstruction of the heavy ion data was
completed in the first half of January. About 200 million additional
Monte Carlo events with classical pile-up event handling were simulated
with the 2018 detector configuration. Over 4.2 billion additional Monte
Carlo events with the 2018 detector configuration and 2 billion events
with the 2017 detector configuration using the resource-efficient,
pre-mixing pile-up method were also simulated and made available to the
analysis teams.]{.c3}

[]{.c3}

[Large parts of the parked physics data from 2018 were staged back from
tape for processing. However, delays in finalizing the small miniAOD
data format delayed start of processing until end of March. For the MIP
Timing Detector TDR a simulation campaign was completed within a few
weeks. An issue in the processing of the forward electromagnetic
calorimeter in the heavy ion (HI) data was identified and requires a
re-processing of 2018 HI data.]{.c3}

[]{.c3}

[CMS reviewed the data on tape and identified datasets that are
superseded. The tape deletion campaign is expected to free about 30
PBytes of space and is currently reviewed by the CMS physics
groups.]{.c3}

[]{.c3}

[Two issues impacted production activities: the EOS/Unified issue
continued to require manual intervention at the beginning of the quarter
and a new EOS/Unified issue a slow down toward]{.c3}

[the end of the quarter. New metadata queries were added to Unified and
overloaded the metadata database service, DBS, in the second half of
March. Identifying the culprit took time and the queries were corrected
end of March.]{.c3}

[ ]{.c3}

[Overall data processing used all available production resources up to
200k cores. During the quarter, up to 30k cores from NERSC and up to 10k
cores from underutilized ATLAS sites were used.]{.c3}

[]{.c3}

[![](images/image1.png)]{style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 341.33px;"}

[]{.c3}

Computing Infrastructure and Services {#h.okdiq8v3dzp4-1 .c8}
-------------------------------------

[The Rucio transition project progressed during this quarter on a number
of fronts. The Kubernetes setup was further refined with regular
synchronization of data between PhEDEx and Rucio. Work began on the code
necessary to interface WMAgent with Rucio, and code was added to DAS to
present Rucio rules. The plans for the transition were revised and
expanded with a planned transition of the NanoAOD data tier first
followed by other data; the updated CMS-wide milestones were presented
at CMS Offline and Computing Week in March 2019.]{.c3}

[]{.c3}

[In this reporting period, the remaining CMS-supported services relying
on CMS topology and other metadata were migrated from the CMS-specific
solution (SiteDB) to a WLCG-supported one (CRIC).  We are now planning
the shutdown of SiteDB in order to free up effort for further
consolidation around CRIC; this shutdown has been delayed to Q2 2019 due
to unavailability (maternity leave) of the CERN-based manager of the
CMSWeb service.]{.c3}

[]{.c3}

U.S. CMS continues to support the CERN-led effort to modernize the
CMSWeb stack.  During this quarter, the Kubernetes-based prototype has
matured and has one outstanding issue which requires resolution by CERN
IT.  Once resolved, we can start the final planning for rollout of this
infrastructure (if no additional issues are discovered, Q3 2019 is
possible).  Finally, the ability for CMSWeb to issue "SciTokens"
([[www.scitokens.org](https://www.google.com/url?q=http://www.scitokens.org&sa=D&ust=1556127443815000){.c6}]{.c5}[)
to access CMS's distributed storage was moved into production.  This
allows continued testing of new authorization mechanisms across the WLCG
infrastructure.]{.c3}

[]{.c3}

[In Q1 2019, additional effort has been started at the University of
Notre Dame for workflow management; this partially alleviates the
shortage caused by the loss of a developer (Ryu) at FNAL during Q4 2018.
 The new developer, Kenyi Hurtado, has been working with Alan Malta to
understand our production software stack.  Hurtada is beginning to do
minor bug fixes and cleanups to better understand the code.  Next
quarter, he will begin on a project to refactor how WMAgent interacts
with HTCondor, improving the scalability of the agent by allowing bulk
submission of jobs.]{.c3}

[]{.c3}

[The original XSEDE allocation ended and US-CMS got renewed allocations
at PSC and TACC. We also received hours at SDSC and Jetstream (XSEDE
Cloud). CVMFS commissiong  continues at TACC, SDSC and Jetstream. NERSC
is being used at good pace and worked to onboard CMS into the HEPCloud
production system.]{.c3}

[Software and Support]{.c15} {#h.okdiq8v3dzp4-2 .c8}
----------------------------

[]{.c3}

[A broad variety of activities have continued in support of both current
CMS operations and long-term preparations for the HL-LHC.  The CMS event
processing framework is being adapted to handle accelerator processors
such as GPUs, allowing for the exploration of pixel tracking on GPUs and
for easier switching between legacy CPU and new GPU algorithms.
 Concurrency is being improved, most recently by modifying the
EventSetup system to allow multiple such  modules to run concurrently
during the same interval of validity update.  Efforts continue on
developing the pre-mixing of pileup events in the simulation, a key
technology for improving the resource needs of that process.  New
developments include the integration of the MTD for HL-LHC studies and
options to allow for the easier production of run-dependent pileup
profiles.]{.c3}

[]{.c3}

[The transition to the community-supported DD4HEP package for the
description of detector geometry is also continuing, with changes to the
package structure that will support the transition, and the integration
and validation of the description of the muon drift tubes.  Geometry
navigation performance has been improved by 75%.  The event
visualization tools are being moved towards a web-based system, with the
first implementation of cross view selection on the client side.]{.c3}

[]{.c3}

[There has been significant progress on the development of
columnar-based analysis tools.  The addition of certain accelerated
functions have sped up one step of the boosted H to bb analysis by a
factor of 125.  New functionality has been added to easily swap among
different histogramming packages.  More users are starting to contribute
code to the effort, expanding the developer base.]{.c3}

[]{.c3}

[A turnkey system for machine learning benchmarking has been developed.
 Two benchmarks are now available, one focused on data analysis and one
on event reconstruction.  Each can run on both GPU and CPU
systems.]{.c3}

[]{.c3}

[Many R&D efforts continue, including exploration of different
technologies for analytics/monitoring efforts; a study of the use of raw
detector information for event classification; and development of
parallel Kalman filter tracking and a segment-linking tracking
algorithm.]{.c3}

[Other activities]{.c15} {#h.okdiq8v3dzp4-3 .c8}
------------------------

[Multiple versions of the reconstruction software were released,
targeted towards the reconstruction of heavy ion data, the
reconstruction of parked data for b physics, simulation and
reconstruction of the MTD for HL-LHC TDR studies, and low-level
reconstruction and calibration for the ultra-legacy reprocessing of Run
2 data.]{.c3}
