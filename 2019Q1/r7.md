\clearpage
# Software and Computing

While LHC accelerator and CMS detector are in Long Shutdown 2 there is no break for Software and Computing. Data processing continues to operate as during LHC running periods, minus the prompt processing and archiving of newly recorded data. The U.S. CMS facilities deployed sufficient computing resources to fulfill the 2019 pledges to CMS, and operational reliability and availability has been excellent, especially for the Fermilab Tier-1 facility. This has enabled use of U.S. analysis computing resources at very high scales, contributing significantly to the physics results shown at various conferences. It also enabled CMS to produce significant numbers of Monte Carlo events in various configurations for running period and detector. This success was also enabled by using High Performance Computing (HPC) resources like the DOE NERSC facility at LBNL and the NSF Pittsburgh Supercomputing Center facility. New allocations for 2019 were granted through the DOE ERCAP and the NSF XSEDE processes, which we intend to efficiently apply to solve CMS physics questions. 

On the side of infrastructure software, the transition to the `Rucio` data management system made significant progress and we upgraded the transition plan. We also migrated the remaining CMS-supported services relying on CMS topology and other metadata from the CMS-specific database (`SiteDB`) to a WLCG-supported solution (`CRIC`). Software development continued with progress in the software framework support for computing accelerators like GPUs and the transition to a community solution for the detector geometry description (`DD4HEP`). The development of tracking on accelerators like GPUs made a lot of progress towards preparing CMS to run efficiently on the future HPC machines of DOE and NSF that typically are dominated by the use of accelerators.

## Major milestones achieved this quarter


-------------------------------------------------------------------------------
Date          Milestone
------------  -----------------------------------------------------------------
Jan 22        Fermilab LPC EOS upgraded to latest version and HTCondor pool re-factored to container based architecture

Jan 25        Release of software for MTD TDR studies

Feb 5         `SciTokens` issuer released to production CMSWeb instance

Mar 9         Release of first version of software for low-level reconstruction and calibration for ultra-legacy processing of Run 2 data

Mar 26        CMS onboarded to HEPCloud

Mar 31        Fermilab Tier-1 site has deployed sufficient processing, storage and archival resources to meet the 2019 CMS resource pledge.

Mar 31        All Tier-2 sites have deployed sufficient processing and storage resources to meet CMS resource pledges.

Mar 31        Updated `Rucio` transition plans presented at CMS Offline \& Computing Week

------------  -----------------------------------------------------------------

## Fermilab Facilities

Throughout this quarter the Fermilab Facilities continued to provide reliable custodial storage, processing and analysis resources to U.S. CMS collaborators.  The site was well utilized, with the facility providing 43.8 million wall-clock hours of processing to CMS.

![Fermilab readiness metrics for Q1 of calendar year 2019. Green indicates a
passing metric, red a failing metric, and blue a scheduled downtime.
The height of a red bar indicates how much of each day Fermilab services were affected.
Black and yellow across the bottom indicates LHC machine running.
During this quarter the LHC was shutdown for the first year of the long shutdown.
Fermilab passed metrics 100% during the quarter.](figures/image2.png){#fig:t1}

Figure @fig:t1 shows the site readiness metrics, and during this quarter the Tier-1 facility passed CMS site availability metrics 100% of the time. Two incidents in this period were caught quickly enough to not impact delivery of computing resources to CMS: In preparation for turning on IPv4/IPv6 on our storage resources the Tier-1 disk dCache had an IPv6 address earlier than needed, causing transfer failures to some sites in March. Also during March the LPC EOS instance was flooded by user requests, causing it to be unavailable for several hours.

In January the Fermilab LPC took a day downtime to successfully upgrade EOS to the latest version, also used by CERN. The HTCondor batch resources used by the LPC users were also migrated from a static node to a more dynamic docker container based architecture, in time for users' preparations for winter physics conference approvals.

## University Facilities

As seen in Figure @fig:t2, CMS production and analysis activities this quarter continued to run at high levels relative to full capacity. Total analysis usage has remained high and the faction of analysis activity by U.S. researchers at our sites has returned to the historical average of \~70% after a lull in activity over the winter.

![Processing usage at the U.S. CMS Tier-2 sites by month, broken down by type of usage (like production, analysis by U.S. researchers, and non-U.S. researchers) compared to purchased processing capacity.](figures/image3.png){#fig:t2}

All of the U.S. CMS Tier-2 sites operated successfully last quarter. On our two official performance metrics based on CMS test jobs, all sites were at least 95%
"[available](https://www.google.com/url?q=http://wlcg-sam-cms.cern.ch/templates/ember/%23/historicalsmry/heatMap?end_time%3D2019%252F04%252F01%252000%253A00%26granularity%3DDaily%26profile%3DCMS_CRITICAL%26site%3DT2_US_Caltech%252CT2_US_Florida%252CT2_US_MIT%252CT2_US_Nebraska%252CT2_US_Purdue%252CT2_US_UCSD%252CT2_US_Wisconsin%26start_time%3D2019%252F01%252F01%252000%253A00%26time%3Dmanual%26type%3DAvailability%2520Ranking%2520Plot&sa=D&ust=1556127443808000)"
and 83%
"[ready](https://www.google.com/url?q=https://dashb-ssb.cern.ch/dashboard/request.py/sitereadinessrank?columnid%3D234%23time%3Dcustom%26start_date%3D2019-01-01%26end_date%3D2019-04-01%26sites%3Dmultiple%26timebins%3Dfalse%26nodata%3Dfalse%26binsselect%3Ddefault%26clouds%3Dall%26site%3DT2_US_Caltech,T2_US_Florida,T2_US_MIT,T2_US_Nebraska,T2_US_Purdue,T2_US_UCSD,T2_US_Wisconsin&sa=D&ust=1556127443808000)",
which is above the CMS requirement for each of these metrics of 80%. The U.S. CMS performance goal is 90%, which all sites met except for Caltech and Purdue. Nonetheless our commitments to CMS were met with success.

The U.S. CMS Tier-2 centers delivered
[49.4%](https://www.google.com/url?q=http://dashb-cms-jobsmry.cern.ch/dashboard/request.py/consumptions_individual?sites%3DT2_AT_Vienna%26sites%3DT2_BE_IIHE%26sites%3DT2_BE_UCL%26sites%3DT2_BR_SPRACE%26sites%3DT2_BR_UERJ%26sites%3DT2_CH_CSCS%26sites%3DT2_CN_Beijing%26sites%3DT2_DE_DESY%26sites%3DT2_DE_DESY_Test%26sites%3DT2_DE_RWTH%26sites%3DT2_EE_Estonia%26sites%3DT2_EE_Estonia_Test%26sites%3DT2_ES_CIEMAT%26sites%3DT2_ES_IFCA%26sites%3DT2_FI_HIP%26sites%3DT2_FI_HIP_Test%26sites%3DT2_FR_CCIN2P3%26sites%3DT2_FR_GRIF_IRFU%26sites%3DT2_FR_GRIF_LLR%26sites%3DT2_FR_IPHC%26sites%3DT2_GR_Ioannina%26sites%3DT2_HU_Budapest%26sites%3DT2_IN_TIFR%26sites%3DT2_IT_Bari%26sites%3DT2_IT_Legnaro%26sites%3DT2_IT_LegnaroTest%26sites%3DT2_IT_Pisa%26sites%3DT2_IT_Rome%26sites%3DT2_KR_KNU%26sites%3DT2_MY_UPM_BIRUNI%26sites%3DT2_PK_NCP%26sites%3DT2_PL_Swierk%26sites%3DT2_PL_Warsaw%26sites%3DT2_PT_NCG_Lisbon%26sites%3DT2_RU_IHEP%26sites%3DT2_RU_INR%26sites%3DT2_RU_ITEP%26sites%3DT2_RU_JINR%26sites%3DT2_RU_PNPI%26sites%3DT2_RU_RRC_KI%26sites%3DT2_RU_SINP%26sites%3DT2_TH_CUNSTDA%26sites%3DT2_TR_METU%26sites%3DT2_UA_KIPT%26sites%3DT2_UK_London_Brunel%26sites%3DT2_UK_London_BrunelTest%26sites%3DT2_UK_London_IC%26sites%3DT2_UK_SGrid_Bristol%26sites%3DT2_UK_SGrid_RALPP%26sites%3DT2_US_Caltech%26sites%3DT2_US_Florida%26sites%3DT2_US_MIT%26sites%3DT2_US_Nebraska%26sites%3DT2_US_Purdue%26sites%3DT2_US_UCSD%26sites%3DT2_US_Vanderbilt%26sites%3DT2_US_Wisconsin%26sitesSort%3D2%26start%3D2019-01-01%26end%3D2019-04-01%26timeRange%3Ddaily%26granularity%3DMonthly%26generic%3D0%26sortBy%3D0%26series%3DAll%26type%3Dewa&sa=D&ust=1556127443810000)
of all computing time by Tier-2 sites in CMS last quarter, down about 1.4% from the previous quarter but consistent with historical averages. While we deliver \~50% of the Tier-2 computing power world-wide, the \~70% analysis usage by U.S. researchers shows that the extra capacity delivered above our headcount n particular goes to supporting data analysis by U.S. researchers, as is its purpose. The storage and processing deployment milestones to meet the April 1, 2019 pledge amounts to CMS were successfullly made.

The U.S. CMS Tier-3 support team provided help to fifteen sites on a number of issues mainly related to OSG software upgrades, Singularity, PhEDEx transfers, XRootD, and HDFS.  We are also preparing for the transition from OSG CA host certificates to InCommon IGTF or alternative CA options.  InCommon certificates have been tested and the Let's Encrypt CA is being explored.  CMS Connect support continued and several interventions were required to handle issues with problematic users causing service degradation.  Access to GPU resources at Vanderbilt were enabled and a workaround to allow interactive access to GPU resources was documented.  Finally, the support team initiated a significant effort to verify the functional status of all U.S. Tier-3 sites and to remove from databases any sites that are no longer maintained.

## Computing Operations

During the long shutdown CMS computing operations continues the processing of Run 2 data and is starting preparations for Run 3.


Distribution, archival, and reconstruction of the heavy ion data was completed in the first half of January. About 200 million additional Monte Carlo events with classical pile-up event handling were simulated with the 2018 detector configuration. Over 4.2 billion additional Monte Carlo events with the 2018 detector configuration and 2 billion events with the 2017 detector configuration using the resource-efficient, pre-mixing pile-up method were also simulated and made available to the analysis teams.

Large parts of the "parked" physics data from 2018 were staged back from tape for processing. However, delays in finalizing the small miniAOD data format delayed start of processing until end of March. For the MIP Timing Detector TDR a simulation campaign was completed within a few weeks. An issue in the processing of the forward electromagnetic calorimeter in the heavy ion (HI) data was identified and requires a re-processing of 2018 HI data.

CMS reviewed the data on tape and identified datasets that are superseded. The tape deletion campaign is expected to free about 30 PBytes of space and is currently reviewed by the CMS physics groups.

Two issues impacted production activities: the EOS/`Unified` issue continued to require manual intervention at the beginning of the quarter and a new EOS/`Unified` issue developed and caused a slow down toward the end of the quarter. New metadata queries were added to `Unified` and overloaded the metadata database service, DBS, in the second half of March. Identifying the culprit took time and the queries were corrected end of March.

Overall data processing used all available production resources up to 200k cores. During the quarter, up to 30k cores from NERSC and up to 10k cores from underutilized ATLAS sites were used.

![](figures/image1.png){#fig:ops}

## Computing Infrastructure and Services

The Rucio transition project progressed during this quarter on a number of fronts. The Kubernetes setup was further refined with regular synchronization of data between PhEDEx and Rucio. Work began on the code necessary to interface WMAgent with Rucio, and code was added to DAS to present Rucio rules. The plans for the transition were revised and expanded with a planned transition of the NanoAOD data tier first followed by other data; the updated CMS-wide milestones were presented at CMS Offline and Computing Week in March 2019.

In this reporting period, the remaining CMS-supported services relying on CMS topology and other metadata were migrated from the CMS-specific solution (SiteDB) to a WLCG-supported one (CRIC).  We are now planning the shutdown of SiteDB in order to free up effort for further consolidation around CRIC; this shutdown has been delayed to Q2 2019 due to unavailability (maternity leave) of the CERN-based manager of the CMSWeb service.

U.S. CMS continues to support the CERN-led effort to modernize the CMSWeb stack.  During this quarter, the Kubernetes-based prototype has matured and has one outstanding issue which requires resolution by CERN IT.  Once resolved, we can start the final planning for rollout of this infrastructure (if no additional issues are discovered, Q3 2019 is possible).  Finally, the ability for CMSWeb to issue "`SciTokens`" ([www.scitokens.org](https://www.google.com/url?q=http://www.scitokens.org&sa=D&ust=1556127443815000)) to access CMS's distributed storage was moved into production.  This allows continued testing of new authorization mechanisms across the WLCG infrastructure.

This quarter additional effort has started at the University of Notre Dame for workflow management; this partially alleviates the shortage caused by the loss of a developer (Ryu) at FNAL during the previous quarter. The new developer, Kenyi Hurtado, has been working with Alan Malta to understand our production software stack.  Hurtada is beginning to do minor bug fixes and cleanups to better understand the code.  Next quarter, he will begin on a project to refactor how WMAgent interacts with HTCondor, improving the scalability of the agent by allowing bulk submission of jobs.

The original XSEDE allocation ended and U.S. CMS got renewed allocations
at PSC and TACC. We also received hours at SDSC and Jetstream (XSEDE
Cloud). CVMFS commissiong continues at TACC, SDSC and Jetstream. NERSC
is being used at good pace and worked to onboard CMS into the HEPCloud
production system.

## Software and Support

A broad variety of activities have continued in support of both current CMS operations and long-term preparations for the HL-LHC.  The CMS event processing framework is being adapted to handle accelerator processors such as GPUs, allowing for the exploration of pixel tracking on GPUs and for easier switching between legacy CPU and new GPU algorithms.   Concurrency is being improved, most recently by modifying the EventSetup system to allow multiple such  modules to run concurrently during the same interval of validity update.  Efforts continue on developing the pre-mixing of pileup events in the simulation, a key technology for improving the resource needs of that process.  New developments include the integration of the MTD for HL-LHC studies and options to allow for the easier production of run-dependent pileup profiles.

The transition to the community-supported DD4HEP package for the description of detector geometry is also continuing, with changes to the package structure that will support the transition, and the integration and validation of the description of the muon drift tubes.  Geometry navigation performance has been improved by 75%.  The event visualization tools are being moved towards a web-based system, with the first implementation of cross view selection on the client side.

There has been significant progress on the development of columnar-based analysis tools.  The addition of certain accelerated functions have sped up one step of the boosted H to bb analysis by a factor of 125.  New functionality has been added to easily swap among different histogramming packages.  More users are starting to contribute code to the effort, expanding the developer base.

A turnkey system for machine learning benchmarking has been developed.   Two benchmarks are now available, one focused on data analysis and one on event reconstruction.  Each can run on both GPU and CPU systems.

Many R&D efforts continue, including exploration of different technologies for analytics/monitoring efforts; a study of the use of raw detector information for event classification; and development of parallel Kalman filter tracking and a segment-linking tracking algorithm.

## Other activities

Multiple versions of the reconstruction software were released, targeted towards the reconstruction of heavy ion data, the reconstruction of parked data for b physics, simulation and reconstruction of the MTD for HL-LHC TDR studies, and low-level reconstruction and calibration for the ultra-legacy reprocessing of Run 2 data.

