# Program Manager's Assessment

Long shutdown 2 of the LHC (LS2) is now well underway. During this shutdown a number of detector improvements are planned, including new electronics for some muon chambers and the installation and commissioning of the hadronic calorimeter barrel upgrade. An extensive program of routine maintenance tasks is also underway.

Refurbishments of the detector proceed well and largely on schedule. The luminosity system is replacing the Pixel Luminosity Telescope detector planes for Run 3,  that were at end-of-life at the end of Run 2. In the Tracker systems, LS2 is an opportunity to improve Single Event Upset performance of the Token Bit Manager, which plagued operations of the Pixel detector during the first half of Run-2. There are several repairs and refurbishments scheduled for the ECAL electronics, including for the large MARATON low-voltage crates. DAQ and Trigger systems also get updated. 

The big ticket items for the U.S. groups during LS2 are the Installation and Commissioning of the HCAL Barrel front-end electronics Phase-1 upgrade, and the refurbishment of the inner-ring Endcap Muon Chambers at each station (MEx/1 chambers), outfitting them with new electronics boards to improve latencies and robustness of triggers. Both programs proceed well as described in the Detector Operations section of this report, despite small issues that are being taken care of as we move forward and that to this date do not impact the overall shutdown schedule. 

There is no break for Software and Computing data processing during the shutdown, with significant processing campaigns proceeding well. Operational reliability and availability of U.S. computing facilties has been excellent, especially for the Fermilab Tier-1 facility that hit 100% in this metric for this quarter. U.S. analysis computing resources are being used at very high scales, contributing significantly to the physics results shown at various conferences. 

CMS continues to produce large Monte Carlo event samples in various detector configurations, including for upgrade studies. Use of High Performance Computing (HPC) resources like the DOE NERSC facility at LBNL and the NSF Pittsburgh Supercomputing Center facility is now becoming routine for the full event simulation chain, and these resources make a significant impact. New allocations for 2019 were granted through the DOE ERCAP and the NSF XSEDE processes, which we intend to efficiently apply to solve CMS physics questions. 

The transition to the Rucio data management system made significant progress, and we migrated remaining CMS-supported services to a WLCG-supported solution. The transition of the CMS detector geometry definition package to a community solution was completed. There was good progress in core software development, in particular support of software access to computing accelerators (in particular GPUs and FPGAs) from the CMSSW framework. There is also good progress in modernizing physics software and algorithms, in particular in optimizing tracking software for running on accelerators like GPUs. This prepares CMS software to run efficiently on future HPC machines, that typically are heavily relying on the use of accelerators.

CMS [published first physics results](doi://xxx) based on the full 137/fb Run 2 data sample just mere months after the last collision data was taken, giving  testimony to the effective operation and high quality of the data.
