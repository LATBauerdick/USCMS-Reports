\documentclass[12pt]{article}

\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}

\usepackage{graphicx}
\usepackage[colorinlistoftodos]{todonotes}


\def\uTCA{\ensuremath{\mu}TCA\,}


\begin{document}




\section{Detector Operations}

Long shutdown 2 of the LHC (LS2) is now well underway.  During this shutdown a
number of detector improvements are planned, including new electronics for some
muon chambers and the installation and commissioning of the hadronic calorimeter
barrel upgrade. An extensive program of routine maintenance tasks is also underway.

During this quarter, there have been three safety related incidents at the experiment.  Neither has directly involved US activities, but are briefly described here.   In the first a small fire occurred in a room housing UPS power supplies.  The safety systems behaved as planned, and the fire brigade was summoned and extinguished the fire.  There was minor damage to the CMS control room resulting from the misting system being trigger by smoke but this has not had a significant impact on work at CMS.  

In the second incident, $\rm CO_2$ was released into the experimental cavern, in 
particular into the Vac tank.  There was no damage or injuries but the incident is
under investigation.  The release occurred in an area where the hadronic barrel
calorimeter (HB) installation is taking place. Fortunately at the time of this
incident there was no one working on HB.  The incident is under investigation and
going forward HB work will not take place while $\rm CO_2$ activities are under
way.

In the third incident, a jig that was being used to lift some shielding broke
causing the shielding to be dropped.
Fortunately proper safety procedures were being followed and no one was under the
crane as the work took place, thus there were no injuries.  This incident is also
being investigated.


Below we summarize the work this quarter by subsystem.


\subsection{BRIL }
During LS2 two PLT detectors are built for the BRIL system. The milestone timeline for the refurbishment has been set aggressively and is on track within a week. All components for the first quarter are tested and ready for mounting. Two laboratories (P5, TIF) were established, for assembly and testing of the new PLTs, and for studying the radiation damage on the old extracted PLT. The production of the re-designed opto-motherboards started. The first two boards arrived and were tested; 13 more need to be built at CERN. @0 more detector planes were sent from PSI; from previous batch bonding on several planes needed to be redone. The availability of DOHs is not a concern anymore. For a subset the connectors have been exchanged. An attenuation test stand has been setup to perform quality tests as they are irradiated and their degradation needs to be quantified. Cold tests for the replacement PLT are in preparation. The freezer failed and needs replacement. One quarter plus full readout will be placed in the freezer box. 
The operation software update and documentation started. The software for luminosity calculation for the collaboration is finalized and continuous feedback to the collaboration is provided via hypernews.


\begin{table}[htp]
\caption{BRIL 2019 Upgrade Milestones}
\begin{center}
\begin{tabular}{|l|p{0.35\linewidth}|r|r|}
\hline
Subsystem&Description&Scheduled&Achieved\\
\hline
BRIL & Assembly of the replacement PLT - first quarter & Jun 30 &  \\
\hline
BRIL & Cold cycle tests done - one quarter & Aug 31 &  \\
\hline
BRIL & Assemble quarters 2,3,4 & Sep 30 &  \\
\hline
BRIL & Certify quarters for full assembly & Nov 31 &  \\
\hline
BRIL & Mechanical assembly for PLT ready (store in cold boxes) & Dec 31  & \\ 
\hline
BRIL & Full Run 2 Luminosity analysis /software review  & Dec 31  &  \\
\hline
\end{tabular}
\end{center}
\label{BRILMIlestones}
\end{table}%

\subsection{Tracker }

We are continuing with LS2 activities and dealing with new problems as they arise. For strips, we continue to be ready to participate in Global Running and are exercising the cosmic ray test stand (CRACK). For pixels, the main emphasis in this quarter has been to start the detector checkout.

\subsubsection{Pixels }

In preparation for the Forward pixel checkout, we managed to break a cooling pipe.
The break cannot easily be repaired with a laser weld or a swap of the cooling pipe
as each option requires detector dis-assembly. A mechanical repair is being pursued both at CERN and Fermilab. Fermilab is also exploring reinforcement techniques for
the cooling connections. We have begun the forward pixel checkout and are
proceeding more slowly than planned on purpose in order to take advantage of any
reinforcement solutions that could come from Fermilab. The main result of the checkout so far is that we found one of the 16 cable bundles, on the first half
cylinder to be tested, broken and are reviving the test techniques to determine where the break
is and the best repair strategy. We also diagnosed a high current condition with
several high voltage channels seen during beam operations. The issue was internal to the high voltage power supply.


\subsubsection{Strips}

The plan for strips is to go colder later this year if possible. Meanwhile, we will keep the strips available for global runs and will continue to monitor detector
conditions. DAQ training and development will continue using the CRACK as well. We are also fixing any service/control/safety issues uncovered during the frequent
power and service interruptions typical in LS2.

\begin{table}[htp]
\caption{Tracker 2019 Milestones}
\begin{center}
\begin{tabular}{|l|l|r|r|}
\hline
Subsystem&Description&Scheduled&Achieved\\
\hline
Tracker & Pixel Phase 1 Detector Removed&Jan 31& Jan 15\\
\hline
Tracker & Strips cold and in global run& June 1& Mar 20\\
\hline
Tracker & Strips tested at -25C &Dec 11&\\
\hline
Tracker & Pixel initial checkout complete &Aug 31 & \\
\hline
\end{tabular}
\end{center}
\label{TrackerMilestones}
\end{table}%


\subsection{ECAL }
In the second quarter maintenance of the detector continued. The replacement of the
HV power connectors was completed ahead of schedule. The network upgrade was also
completed on schedule. The refurbishment of the laser barracks began following the
transparency measurements earlier in the year. The floor requires reinforcement to
carry the weight of the optical tables. A plan was made for this. The laser system
is also in the process of being disassembled for a deep maintenance including
sending the lasers to the manufacturer. 

Extensive work has gone into generating a final set of calibrations for the legacy
papers from the 2016-17 run. This is known as the ``ultra legacy re-reco". During
the quarter the calibrations were completed. The re-reco  gives significant
improvement in the resolution particularly in the end-cap where the effects of aging are more significant.

The ECAL group met for its annual meeting "ECAL days" at ETH in Zurich. A
comprehensive review of all aspects of the detector operations during LS2 and
preparations for LS3 was conducted with two full days of presentations. 

\begin{table}[htp]
\caption{ECAL 2019 Milestones}
\begin{center}
\begin{tabular}{|l|p{0.35\linewidth}|r|r|}
\hline
Subsystem&Description&Scheduled&Achieved\\
\hline
ECAL & Complete First Laser Maintenance   & Sep 30 (April 30) &  \\ \hline
ECAL & Complete Second Laser Maintenance & Dec 31 (July 31) &   \\ \hline
ECAL & Complete infrastructure update of laser barracks  & Dec 31 &  \\ \hline
ECAL & Complete replacement of all Maraton input/output Cooling Pipes &  June 30 & April 1 \\
\hline
ECAL & Complete replacement of input HV power connectors  & Sept 30& May 15 \\
\hline
ECAL & Complete modification of CANBUS and update DCS software  & Oct 31& April 1 \\
\hline
ECAL & Complete VME PC replacement  & Mar 31& March 15 \\
\hline
ECAL & Complete network upgrade  & May 31& May 31 \\
\hline
ECAL & Complete upgrade to XDAQ15  & Dec 31&  \\
\hline
ECAL & Complete  installation of new water pump at surface  & Aug 31&  \\
\hline
ECAL & Complete insulation of surface to underground area pipes  & Dec 31&  \\
\hline
\end{tabular}
\end{center}
\label{ECAL2018Milestones}
\end{table}%



\subsection{HCAL}

During the second quarter of 2019, the HCAL Operations group focused on completing
the remaining components of the HCAL Barrel (HB)  Phase 1 Upgrade electronics and on
continuing the HB Phase 1 Upgrade installation.

\subsubsection{HB Installation and Commissioning}

The HB Phase 1 upgrade, 
planned for LS2, is progressing well.
The removal of all the Phase 0 electronics from HB is complete. 
All Phase 1 backplanes have been installed.
All Phase 1 Clock and Control modules (CCMs) and  all Phase 1 Calibration Units (CUs) have been 
installed.

The two issues came up during the burn-in 
of the Phase 1 HB electronics earlier this year have been resolved.
The first was that a number of the SiPM control card mezzanines were of sub-standard quality
due to problems with the assembly process. The boards were remade by Fermilab. They have been shipped
to CERN, tested, and re-installed. 
The second was that a very small number of SiPM mounting boards ( $< 0.2\%$) showed an increase in leakage current 
when being burned-in. The affected channels were near the edges of the boards. The problem was
identified, an improved design made, and modified boards built by Fermilab. 
 All the new boards have arrived at CERN and are being installed in the Readout Modules (RM). 96 of the new boards have already been installed in RMs and the testing of
 the first set of 48 re-worked RMs has begun. All RM will be burned in for two weeks before 
 being installed in the detector.

  
 Work on both ends
 of the HB has been progressing in parallel rather than completing
 the HB minus end first. This was done in order to accommodate the need
 to re-make the SiPM control card mezzanines and mounting boards
 as described above.  Even with these two issues, it is expected that the HB Phase 1 Upgrade installation
 can be completed by the end of 2019.  The current schedule still has 8 weeks of contingency remaining.\\

%On June 5 there was an incident where a substantial amount of CO2 was released into the
% vacuum tank during tests of the new pixel cooling system. This includes the area where HCAL personnel are installing the upgraded HB electronics. Fortunately no personnel were in the
% area and there were no casualties and no damage. A Technical Incident Panel has been
% established to understand what went wrong and to prevent such incidents from happening again.\\



In table~\ref{HB Upgrade Milestones}, all the needed HB upgrade electronics are abbreviated ``HBE''.


%\todo[inline]{HCAL Operations summary here}

\subsubsection{HCAL Operations }
Although the HCAL generally performed well during the 2018 running, there were two significant issues.
First, there was an over voltage transient that occurred during the the power-on of CAEN A3100HBP modules 
on June 30 after a CERN wide power cut that caused
irreversible damage of two sectors on the HE minus side, HEM15 and HEM16. 
Second,
HCAL experienced disruptions 
to the quality of specific primary control links between the front-end controllers (FECs) in the CMS service cavern 
and the clock, control, and monitoring modules (CCMs) on the detector for HEP10 and HEM9.
Redundant (secondary) control links were enabled for these detectors and allowed 
HCAL to continue taking high quality data. However, the primary control links resumed
working correctly about a month later, and continued working until the end of the run.
The affected electronics for HEM15 and HEM16 as well as HEP10 and HEM9 have now
been removed from the detector. 
The DC/DC converters on HEM15 and HEM16 were found to be damaged and these
modules have now been successfully repaired. The  HEP10 and HEM9 CCMs
have been removed from the detector and are being tested in Building 904. Depending on the
results of the tests, either those CCMs or spares will be installed back in the detector.
The low voltage power supplies will be shipped back to CAEN for modification
to prevent future over voltage transients during power-on and a protection circuits
for both HE and HB will be designed and installed during 2019.
CAEN has sent a prototype protection circuit to CERN which was installed in a
power supply and  successfully tested. Note that  an ad-hoc protection circuit
was quickly added to HE after the June 30, 2018 incident to prevent problems from transients, and
CMS will be able to continue to operate the current low voltage supplies safely before they are
modified.


Also, the FEC and CCM firmware has been upgraded to be compatible with
half-speed control links. This will ease timing and decrease the Single Event Upset probability.




\begin{table}[htp]
\caption{HB Upgrade Milestones}
\begin{center}
\begin{tabular}{|l|l|r|r|}
\hline
Subsystem&Description&Scheduled&Achieved\\
\hline
HCAL & HBE assembly starts at CERN & 1-Sep-2018 &  1-Sep-2018  \\
\hline
HCAL & HBE production in ``factory mode'' & 15-Nov-2018 &  20-Nov-2018  \\
\hline
HCAL& HBE IRR passed & 15-Feb-2019 &  15-Feb-2019 \\
\hline
HCAL& HBE Minus end upgrade installation begins & 19-Feb-2019 & 18-Feb-2019 \\
\hline
HCAL & HBE production complete & 28-Feb-2019 & see text  \\
\hline
HCAL& HBE Minus end Upgrade Complete & 30-Jun-2019  & see text   \\
\hline
HCAL& HBE Plus end upgrade installation begins & 1-Sep-2019  &  \\
\hline
HCAL& HBE Plus end Upgrade Complete & 20-Dec-2019   &   \\
\hline
\end{tabular}
\end{center}
\label{HB Upgrade Milestones}
\end{table}%

\begin{table}[htp]
\caption{2019 HCAL Operations Milestones}
\begin{center}
\begin{tabular}{|l|l|r|r|}
\hline
Subsystem&Description&Scheduled&Achieved\\
\hline
HCAL & extract HEM09 and HEP10 CCMs  & 10-Feb-2019 &  30-Jan-2019  \\
\hline
HCAL & extract HEM15 and HEM16 CCMs, RMs, CUs & 15-Mar-2019 &  30-Jan-2019  \\
\hline
HCAL&  finalize half-speed FEC and CCM firmware  & 1-Apr-2019 & 15-Mar-2019 \\
\hline
HCAL& reinstall HEM09, HEP10 modules & 1-Jun-2019 &  see text  \\
\hline
HCAL& reinstall HEM15, HEM16 modules & 1-Jun-2019 &  see text  \\
\hline
HCAL & recommission HE with half-speed control links & 1-Jun-2019 &  15-May-2019  \\
\hline
HCAL& LV PS back to CAEN for modification starts & 1-Jun-2019  & see text  \\
\hline
HCAL& new LV protection circuits installed & 1-Sep-2019  &  \\
\hline
\end{tabular}
\end{center}
\label{2019 HCAL Operations Milestones}
\end{table}%



\noindent

\vfill






\subsection{EMU }


The major CSC activity in Long Shutdown 2 continues to be the refurbishment with updated electronics of 180 chambers in the inner rings of the CSC system.  Chambers are extracted from CMS and brought to the surface at SX5 where the older electronics boards are stripped off, new ones installed.  The refurbished chambers are subjected to a battery of  tests in electronics tests stand in SX5, and then monitored under power on a long-term-test stand.  After a leak test of the cooling circuits, the chambers are reinstalled in CMS and await recommissioning.   The work of chamber refurbishment and testing, infrastructure installation and re-commissioning is distributed among small teams that are composed mostly of graduate students and are led by senior scientists or postdocs. 

The first ring of chambers refurbished was ME-1/1 (the inner ring of the first station on the negative YE-1 end cap), which comprises 36 chambers. The chambers in this ring required the old digital cathode front end boards (DCFEBs) to be replaced with new DCFEBv2 boards, and the ALCT mezzanines to be replaced with new ALCT LX100 boards.   All 36 chambers were extracted to the surface during the previous quarter.  In April through June, the refurbishment, testing, and re-installation of all 36 chambers was completed, and the final chamber was re-installed on 14 June. As of the end of June, 15 chambers have been fully reconnected, and twelve of these have completed full re-commissioning with cosmic rays. 

The cooling circuit of each chamber is tested for leaks before re-installation.  One of the first ME1/1 chambers to be tested (ME-1/1/33) was found to have developed a leak at one of the joints. This chamber was refitted with a new circuit, and the problem was studied by CMS Technical Coordination and expert engineers from the CERN main machine shop, and a plan was developed for minimizing the risk of future leaks.
For ME-1/1, we decided not to attempt a systematic repair of all the cooling circuits of the chambers now and only perform a repair of circuits which fail the leak test in SX5. In addition, the refurbishment procedures were altered to minimize any possible stress on the cooling circuits, and an individual leak detection cable was installed in each chamber during refurbishment. Aside from the first chamber that failed, all of the ME-1/1 chambers passed stringent leak tests, and no other repairs to the cooling circuits were required. For longer term strategy the CERN central shop has agreed to design and construct 72 new cooling circuits with a design which does not use any connection joints. According to the CMS LS2 schedule 36 of those circuits will be needed by Oct 1st, 2019, in time for the start of the ME+1/1 electronics upgrade. The CERN central shop has promised to delivery on time the first batch of 36. The additional 36 will be installed on ME-1/1 at the earliest opportunity and not later than LS3.

Large-scale assembly of the DCFEBv2 boards has continued since January.  As of the end of June, 450 of the 585 boards (including spares) have been assembled.  The production and testing is expected to conclude early in the next quarter.  252 DCFEBv2 board were installed in the ME-1/1 ring, and 45 will be installed in the ME+2/1 ring during the summer.  Finally, 252 will be installed in the ME+1/1 ring starting in the Fall.  The quality of the boards is very high.  

The ALCT LX100 boards (needed for ME1/1, ME1/2, ME2/2, ME3/2)  completed production in May.  The production of ALCT LX150T board (needed for ME2/1, ME3/1, ME4/1) was started in mid-June after a two-month delay for financial transfers.   The fabrication is complete, and assembly is expected to finish in mid-July.


The assembly of the new low-voltage infrastructure (supplies, cables, junction boxes) continued through this quarter.  The installation work on the plus side is about half-way finished and is expected to require most of July to complete.  

The new optical fibers for ME2/1, ME3/1, ME4/1 were all tested at the vendor and underwent sample testing at CERN to verify the quality.  The fibers are ready for installation. 

The old DCFEB boards (252) extracted from ME1/1 were all retrofitted with VTTx optical transmitters.  This operation was carried out by our colleagues from Tomsk State University (RU).  In addition, four 10 $\mu$F capacitors on each DCFEB are being replaced to reduce the noise on the 2V line. These boards will be installed in the ME+2/1, ME+3/1 and ME+4/1 chambers during the summer.

New Optical Trigger Motherboards (OTMB) are required for ME234/1 once the DCFEBs are in place.  These boards consist of a VME base board plus a mezzanine that carries the FPGA and optical transceivers.  Five prototypes of each were completed and tested in April and May.  The production readiness review for the OTMB is on 28-June.  

In general, the refurbishment has gone as planned aside CMS-wide delays due to major power outages and the halt of access after the UPS fire (described in the introduction).  The overall schedule is now shifted by about three weeks from the original schedule. Most milestones are on track (see table), and mitigation plans are in place for those that have slipped.  The general schedule delays for CMS have had an impact on the milestones for ME-1/1 commissioning (now foreseen for 8 July) and LV power for ME+234 (now foreseen for late July).  Financial fund transfer issues delayed the start of production for ALCT-LX150T boards, and these are now expected in late July.  This might require some re-arrangement of the commissioning and installation plans for ME+2/1.

In summary the activity to install new electronics on CSC chambers has completed the first ring of 36 chambers, and the work on the second ring has begun.


\begin{table}[htp]
\caption{EMU 2019 Milestones}
\begin{center}
\def\arraystretch{1.5}

\begin{tabular}{|l|p{0.5\linewidth}|r|p{0.18\linewidth}|}
\hline
Subsystem&Description&Scheduled&Achieved\\
\hline
EMU-MEX/1 & CSC LV junction boxes ready for installation & Jan 15 &  Feb 1\\
\hline
EMU-MEX/1& First ME-1/1 chamber extracted to SX5 & Feb 25 &  Feb 25 \\
\hline
EMU-MEX/1 & CSC DCFEBv2 boards ready for installation on ME-1/1  & Mar 11 &  Mar 7\\
\hline
EMU-MEX/1 & ALCT-LX100 ready for installation in ME-1/1 & Mar 11 &  Mar 7\\
\hline
EMU-MEX/1 & Full production of ALCT-LX150T released  & Apr 15 & Jun 12 \\
\hline
EMU-MEX/1 & CSC on-chamber optical fibers ready for installation & Apr 15 &  May 8\\
\hline
EMU-MEX/1 & ME-1/1 installed and commissioned  & Jun 2 &  see text \\
\hline
EMU-MEX/1 & ALCT-LX150T ready for installation in ME+234/1 & Jun 10 & see text \\
\hline
EMU-MEX/1 & First ME+234/1 chamber extracted to SX5 & Jun 17 & Jun 20 \\
\hline
EMU-MEX/1 & New LV power in place for ME+234 & Jun 30 &  see text\\
\hline
EMU-MEX/1 & ALCT-LX100 ready for installation on ME2/2,3/2& Jun 30 &  May 17\\
\hline
EMU-OPS/1& LV power in place for plus end cap & Aug 8 &  \\
\hline
EMU-OPS/1& Ready to join MWGR with ME-1/1 & Sep 1 &  \\
\hline
EMU-MEX/1 & First ME+1/1 chamber extracted to SX5& Oct 28 &  \\
\hline
EMU-OPS/1& Ready to join MWGR with ME+234/1 & Nov 1 &  \\
\hline
EMU-MEX/1 & OTMB production complete & Nov 15 &  \\
\hline
\end{tabular}
\end{center}
\label{EMUMilestones2019}
\end{table}

\subsection{DAQ}
The run-2 DAQ system was fully operational at the beginning of the reporting period.
The DAQ system has been used for commissioning activities by subsystems.
The pixel clean-room DAQ was separated from the run-2 DAQ testbed and reconfigured as standard miniDAQ setup.
The retired HLT nodes were relocated to make room for the future core services in the racks power by UPS. They are permanently assigned to the online cloud and provide 540\,kHEPSpec from 24k physical cores. 
The latest generation HLT nodes were used to process offline workflows whenever commissioning activities allowed to. They provide an additional 310\,kHEPSpec from 13k cores. The online cloud processed about 1.5 million offline jobs per week with a 98\% success rate.
All online cloud nodes were migrated to CC7.6 and the latest Openstack Ocata release.

The DAQ system was switched off on May~17 in anticipation of the electrical maintenance work affecting all systems at pt.5. After the UPS incident described above,  the cleanup and reinstating of the fire and UPS protection has delayed the powering back of the upstairs systems until July~1.
This inhibited the use of the miniDAQ systems used for commissioning activities. It slowed down the event-builder development for the run-3 DAQ. And the offline computing plans had to be adjusted to work around the missing online cloud computing resources. 
A temporary control room was set up by the sys-admin team in the SCX work/printer room 3562. This allowed to resume the shift coverage during daytime and commissioning activities underground.
On the surface only the essential systems (network, Oracle database and DCS) were brought back online, but they have not been backed by UPS.

The preparations for run 3 continues despite the limited availability of the run-3 testbed and the additional workload on the sys-admin team.
The order for new computing blades for DCS and cluster services has been sent out.
The specifications for the new VMEPCs have been prepared and the tender process has been launched.
The detailed planning of DAQ commissioning and interruptions of services during LS2 is in progress.
Tests of new storage manager hardware with a company have been done using a benchmark application which emulates the merging and transfer workload. A few open questions need to be follow up with them. We also need to converge on requirements (for proton and heavy-ion running) by late summer.

%The run-3 testbed (daq3val) has been updated to XDAQ pre-release 15 with gcc-8 (C++14).
%The event-builder software is making use of the new language features which improves the readability of the code and also increased the performance a bit. Further work is being done to speed up the writing of events to the RAMdisk and the subsequent access to the files from the HLT nodes. The newer kernel series 4, which will be part of the future CentOS8, improves the disk-writing performance considerably thanks to the use of huge memory pages for the RAMdisk.
%Initial measurements with deep-buffer Ethernet switches shows that RDMA over Converged Ethernet (RoCE) could be a viable alternative to native Infiniband for the event-builder network. 

We pursue the possibility for a heterogeneous HLT farm for run~3. Initial work has shown that the goal of offloading more than 20\% of the HLT CPU to GPUs is realistic for the beginning of run 3. This would make the option to equip each HLT node with NVIDIA GPUs also financially attractive. A corresponding proposal has been made to CMS management. The final decision on this has to be taken in September 2020 before the tendering process for the new HLT nodes gets underway.

%We are looking together with the AlCaDB group into the possibility to update detector condition data used by HLT at luminosity-section boundaries instead of only at the start of a run. This would obliterate the need to inject conditions data into the event stream as done by the obsolete SCAL hardware. A prototype implementation has been tested in daq3val and using the full HLT farm. 

The OMS project is making good progress on restructuring the underlying database tables. The main issue is to converge on the data inputs, where we have to rely on work done by other groups in CMS.
The transition of the subsystem specific pages from WbM to OMS is in full swing. The additional functionality and use cases from these pages require some design changes in the user-interface back end. These changes are being implemented in a timely manner.
We continue to work on a new scheme on how to define and report the down- and deadtime accounting of CMS.

\begin{table}[htp]
\caption{DAQ 2019 Milestones}
\begin{center}
\begin{tabular}{|l|p{0.4\linewidth}|r|r|}
\hline
Subsystem&Description&Scheduled&Achieved\\
\hline
DAQ &Migrated all DAQ s/w to gitlab	&Feb 1 &Feb 1 \\
\hline
DAQ &RCMS and XDAQ releases from gitlab deployed &Mar 15 &Mar 15  \\
\hline
DAQ &Core XDAQ packages built with C++11/14 &Jun 1 &Apr 9 \\
\hline
DAQ &Evaluated storage-manager h/w for run 3 &Aug 1 & \\
\hline
%DAQ &First DTH prototype P1 board &Oct 1 & \\
%\hline
DAQ &OMS DB tables fully replace legacy tables from WbM/SCAL &Oct 1 & \\
\hline
DAQ &Central DAQ hardware choices finalized &Nov 1 & \\
\hline
\end{tabular}
\end{center}
\label{DAQ2019Milestones}
\end{table}%

\subsection{Trigger}

%\todo[inline]{Trigger top level summary here}

During this quarter the US groups continued their maintenance,
operations, and development work on the Layer-1
calorimeter (CaloL1) trigger, endcap muon trigger (EMTF), and global
Level-1 trigger systems, as well as
on the field operations of the Trigger Studies Group. These
preparations will
provide improvements and reliable running during Run 3 data taking
operations.

\subsubsection{Endcap Muon Trigger}

%\todo[inline]{EMTF summary here}

An introduction and training session on the EMTF system was recently
held to help train a new generation for studies and operations. A
total of  15 people participated including new graduate students,
undergraduates, and a postdoc.  Various studies continue in order to
prepare the EMTF for improved trigger performance 
during Run 3. This includes the
optimization of the compression of the number of bits to encode the
bend angles from CSC + GEM for improved PT resolution, and a 
further characterization of the beam halo background to a potential
displaced muon algorithm.



\subsubsection{Layer-1 Calorimeter Trigger}

%\todo[inline]{Layer 1 CALO trigger summary here}

The L1Calo smoothly participated in
the first Midweek Global Run (MWGR), March 20-22. 
During the second quarter, the preparation for
removal of the old RCT cards continued, and the first cards and crates should be
removed and lifted up to the surface in July-August. The exact dates are
under discussion now. The place for storing the parts that can be used later is
prepared.  

The preparation for the second MWGR 2019 was done successfully, but the MWGR
was canceled because of some problems in P5 general operation.
The L1Calo group is working on modifying the calibration procedures that should
improve performance of the calorimeter trigger in Run3 and also few checks 
are schedule for the SWATCH software update. These checks will be done 
during the next MWGR.

\subsubsection{Global Trigger}

Work on pileup mitigation in the Level-1 Trigger  involved first learning how to implement neural networks into FPGAs.
Work accomplished so far includes training of a graph NN for muon track finding, pruning the network using an L1 regularization technique, and implementing the graph NN for an FPGA via hls4ml.  Work is still in progress.  

\subsubsection{Field Operations Group of the Trigger Studies Group}

%\todo[inline]{FOG summary here}
A student has recently been stationed at CERN for the summer in order to be trained on HLT operations from the senior  FOG members. Work continues on improving the rate monitoring tools. Another student is utilizing Phase-1 L1 Trigger DQM modules as a starting point for 
the development of DQM modules for the L1 Trigger Phase 2 sequence
(muons, calo, tracks). This will
soon shift to collaboration with the DPGs for Phase 2 specific
quantities.  




\begin{table}[htp]
\caption{Trigger 2019 Milestones}
\begin{center}
\begin{tabular}{|l|p{0.5\linewidth}|r|r|}
\hline
Subsystem&Description&Scheduled&Achieved\\
\hline
TRIG &Architecture for bringing GE1/1 signals to EMTF specified	&Aug 30 & \\
\hline
TRIG &EMTF online software framework extended to include GE1/1 &Aug 30 &  \\
\hline
TRIG &Neural network PT assignment implementation into EMTF firmware&Dec 31 & Jun 30\\
\hline
TRIG &Initial algorithm to include GE1/1 into EMTF &Dec 31 & \\
\hline
TRIG &Remove legacy BMTF firmware and keep only Kalman filter &Dec 31 & \\
\hline
TRIG &Modify the Kalman filter algorithm to use the upgraded trigger primitives &Dec 31 & \\
\hline
TRIG &Deinstall old RCT hardware &Sept 30 & \\
\hline
TRIG &Exchange microSDs in all CTP7 cards &Sept 30 & \\
\hline
TRIG &Calo Trig: Fix the cable trays in P5 &Sept 30 & \\
\hline
TRIG &Calo Trig: Initial demonstration of pileup mitigation algorithms involving machine learning using High-Level Synthesis &Jul 1 & \\
\hline
TRIG &Calo Trig: Performance results of pileup mitigation algorithms involving machine learning  &Dec 31 & \\
\hline
TRIG &Assess needs for any rate monitoring software developments &Dec 31 & \\
\hline
TRIG &Update rate monitoring software to adapt to evolution in DAQ infrastructure &Dec 31 & \\
\hline
\end{tabular}
\end{center}
\label{TRIG2019Milestones}
\end{table}%

\end{document}