# Software and Computing

As the LHC has settled into stable operations this quarter, the Software and Computing also entered a operational mode.  Various aspects of operations, like central processing and data management, have become routine, with adjustments as experience is gained. Stable and robust operations is facilitated by advances in computing and software that resulted from the various development efforts carried out during LS1.  While development continues on many products and services, the top priority is now to support computing operations, with an eye towards the use of new resources such as amazon.com cloud computing or clusters at supercomputing centers.  CMS software performance benefits from the multi-threaded framework, and development continues to expand the scope of those gains.  As a result, facilities at Fermilab and the universities have been increasingly utilized, and resources have been much more full than in previous quarters, while site performance has remained good.  End-of-year hardware purchases will help to keep up with the increasing demands. 

Date            Milestone  
--------------  --------------------------------------
July 2015       Production release CMSSW\_7\_5\_0 made available 
August 2015     Implement continuous integration for WMAgent
September 2015  Tier-1 purchases completed for 2015

Table: Major milestones achieved this quarter

## Fermilab Facilities

Run 2 data taking continued through the quarter. The Fermilab facility received over a petabyte of LHC collision data so far. Utilization of the Tier-1 has increased and the Fermilab facility was normally full to capacity.  Site readiness improved over the previous quarter, and Fermilab passed CMS site tests 98% of the time, as shown in Figure {@fig:f1}. 

![Fermilab readiness metrics for 2015Q3. Green indicates a passing metric, red a failed metric. Blue indicates a scheduled downtime. Fermilab was 98% available during the quarter.][image-1]

The 2015 Tier-1 equipment purchase cycle was completed. New hardware began to arrive and be installed late September. We were able to take advantage of very good market pricing this year both for disk and CPU, and used this as an opportunity to buy-ahead equipment for the next years, when significant additional budget pressure is expected.  Fermilab purchased 168 nodes of 48-core Intel CPU batch nodes, and approximately 8 PB of disk storage toward the 2016, and to some extent 2017, CMS pledges. The increased size of the batch farm required upgrades in network connectivity to the storage pools, and management reserve funds were used to facilitate these procurements.  About 6 PB of tape media was purchased and installed to hold incoming Run 2 data.  


## University Facilities

This quarter saw a continuing increase in the use of the U.S. CMS Tier-2 facilities, as shown in Figure {@fig:f2}. The increase was largely due to running part of the  CMS data reconstruction at U.S. Tier-2 sites.  This workflow has been possible at Tier-2 sites since May, enabled partly by development work undertaken during LS1.  These workflows place a heavy strain on the site's internal networking capabilities, but all U.S. sites are able to handle the increased load. Physics analysis with CRAB3 is also increasing.

The seven U.S. Tier-2 sites are planning or are in the process of finalizing their hardware purchases for 2015. The connection of the Tier-2 sites to the LHCOne VPN provided by ESNet is proceeding as planned and is completing this quarter. All sites have deployed the HTCondor-CE computing element, and have either retired or are planning to retire their GRAM CEs very soon.

All U.S. CMS Tier-2 sites are operating successfully, in terms of the performance metrics based on CMS test jobs. All sites were at least 92.4% available (corresponding to a 4.4% improvement over the third quarter 2015) and 95% ready, which is 1% point better than last quarter. The goal for both of these metrics is 80%. The 7 U.S. sites delivered 44% of all CMS Tier-2 computing time (the goal is to be \>25%) and are among the 8 top most-used Tier-2 sites in CMS.

![Wall clock time consumption of CMS workflows at the U.S.CMS Tier-2 sites by month.][image-2]

Fourteen Tier-3 sites required assistance from the U.S. CMS Tier-3 support team this quarter.  Support activities include helping sites complete the transition from OSG 3.1 to 3.2, which had a critical security patch for GUMS that could only be applied to  upgraded sites. In addition, the site support team  continues to assist several sites in rebuilding their site in preparation for Run 2.  In particular, there is an ongoing effort to implement a modern cluster configuration and provisioning system at the University of Maryland, as well as new efforts to rebuild clusters at Rice and FSU.  Efforts continue to refresh documentation for site configuration and administration.  

Progress on CMS Connect focused on integrating the infrastructure with CMS monitoring, which required significant software development. Documentation is nearly complete and beta testing should begin during the next quarter.

## Operations

The Tier-0 operated in data-taking mode, and prompt processing of all freshly recorded data was successful and proceeded without significant problems.  Smaller issues and software changes were quickly taken care of. The core Tier-0 software developer and co-leader (D.Hufnagel) is moving on helping with other development projects as the Tier-0 enters an operations-dominated period.

Activities in Monte Carlo production and reprocessing have been relatively low, so the production systems kept up with incoming requests. Streamlining of production and processing activities continued. The operations team was well prepared to deal with production campaigns which started in the beginning of October. In particular, the integration of the new dynamic disk space management tool for production data samples will be very valuable to help staying within quotas. In this quarter we completed 2.44B DIGI-RECO events, 3.38B GEN-SIM events and re-done 50M MINIAOD. The last number is expected to increase dramatically in the next quarter as we are have just started the re-reconstruction campaign. Running production requests on  HLT resources will be a topic for the next quarter as use of HLT for data taking was priority this quarter.

There was substantial progress on data transfers and data management. Integration of AAA operations continued, and while there are still issues reported regularly the operations team is getting more experienced in solving problems. The Dynamic Data Management (DDM) system was deployed and started automatic deletions of data sets in the production data space. The system protects data from deletion by a simple locking mechanism, and any data in use by production at any site has to be explicitly locked. Thus any production activity that requires data has to set and release locks, either automatically or by hand. Integration of DDM into all production activities will continue for some time before it can be considered complete. In the next quarter we expect to review the DDM policies and continue development towards further optimizing space usage. Tape pledges at the Tier-1 centers have been reviewed and we are continuing our deletion campaigns to free up space on existing tapes. We plan to use the new DDM tools to help reduce the effort required for the deletion campaigns.

## Computing Infrastructure and Services

Work centered around supporting Run 2 data taking and continuing to lay the groundwork for future improvements. All projects made progress on modernizing code.  

Work on WMAgent concentrated on continued small improvements and the addition of new workflow types and features to support running on Amazon Web Services (AWS), including a new chained workflow type. The WMAgent team completed a campaign to modernize its Python code and automated testing of changes in continuous integration. The transition to version 2 of  Request Manager continues and is expected to be  complete by the first quarter in 2016.

With development effectively finished the Tier-0 entered a period of operation during data taking. Updates included production of the MINIAOD data tier and supporting the automatic data management.  

Development responsibilities for CRAB3, initially done by U.S.CMS, have almost completely shifted to international CMS. CRAB3 usage continued to grow With users migrating from CRAB2.  

The CMS metadata services DAS and DBS served ever larger numbers of requests with the beginning of the run began, requiring some changes to enable the larger load. For DAS we implemented horizontal scaling to additional servers. DBS struggled with memory consumption issues that are being solved with Python generators, and the new code will be moved into production in the next quarter.  

There was a major GlideinWMS release during the quarter and two patch releases. The former provides the important ability to schedule an additional high-IO job in a multicore pilot. This is crucial for the Tier-0 to avoid scheduling too many high-IO jobs on a single physical machine or leaving resources unallocated. This will be put into production in the next quarter. In the future, GlideinWMS will improve the monitoring related to completed multicore glideins.  Coupled with the AWS pilot project, the GlideinWMS team will add native configuration support for EC2 spot pricing and availability zones in the glidein factory.  


Efforts continue to increase the operational robustness of the remote data access (AAA) infrastructure.  A number of issues in the stability of the regional redirectors were identified and fixed; along the way many sites were upgraded to newer versions of Xrootd, that provided a number of improvements.  Further improvements to monitoring and testing are being considered, and the transitional federation for less-performant sites will be fully populated in the coming quarter.

We continue to make progress on cloud and opportunistic resource usage. We completed our computing allocation at SDSC, using 1.8M core hours on the Gordon machine, roughly equivalent to 4M core hours at CMS Tier-2 centers.  This allocation was used for GEN-SIM as well as DIGI-RECO jobs. For the latter, the pileup samples were read via XRootd from the UCSD Tier-2, and the GEN-SIM inputs for the DIGI-RECO step were read via Xrootd from Fermilab. Output data was staged out directly to the Fermilab storage systems and file merging was run at Fermilab. This was an improvement to the previous use of Gordon in 2013 requiring no significant local disk space at Gordon and thus simplifying operations significantly. 

There are two projects that aim at establishing a new capability to elastically grow computing resources on  demand. For use of cloud computing resources we collaborate as part of the Fermilab HEPCloud in the amazon.com Web services (AWS) project. For future use of High-Peformance Computing (HPC) centers like NERSC and Comet, we collaborate with OSG working through the remaining issues in using these resources.

## Software and Support

The Software and Support group continues to contribute to improving software. For data taken in 2015 the CMSSW\_7\_4\_X release series is being maintained as the main production release.  Introduction of multi-threaded reconstruction resulted in large memory savings and reduced processing latency, enabling to handle the high Run 2 trigger rates.  During software commissioning on the Tier-0 system it was found that its use of a very large number of output modules and just-in-time compilation within ROOT causes larger concurrency inefficiencies than expected in testing. These problems were fixed in a new development release series CMSSW\_7\_6\_X, which will be used for the end-of-year reprocessing.  

Maintenance work was also done on the 7\_4 release to allow it to be used in the HLT, which is currently running in single threaded mode. Due to a lack of sufficient memory per core it cannot sustain the highest trigger rate that occurred during the last two weeks of data taking without moving to multi-threaded processing.  In Figure {@fig:f3}, the point circled in black marks the current limit of the single-threaded HLT application, and the green circle marks the rate that the new multi-threaded application allows. This new capability comes just in time to maximize physics data taking efficiency for CMS.

![Event throughput rates as a function of the number of threads for single- and multi-threaded applications.][image-3]


Meanwhile the CMSSW 7\_5\_X release series is being prepared to operate during the upcoming heavy-ion run. For this release the GEN-SIM application was validated to run effectively in multi-threaded mode.



[image-1]:	figures/image02.png {#fig:f1}
[image-2]:	figures/image01.jpg {#fig:f2}
[image-3]:	figures/image00.png {#fig:f3}