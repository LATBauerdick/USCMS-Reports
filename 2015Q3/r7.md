# Software and Computing

edited in Editorial -- 1Writer -- Sat

As the LHC has settled into stable operations this quarter, the Software and Computing also entered a operational mode.  Various aspects of operations, like central processing and data management, have become routine, with adjustments as necessary as experience is gained.  Stable and robust operations is facilitated by the advances in computing and software as a result of various development efforts carried out during LS1.  While development continues on many products and services, the top priority is supporting current operations, with an eye towards the use of new resources such as the amazon.com cloud or clusters at supercomputing centers.  CMS software performance benefits from the multi-threaded framework, and development continues to expand the scope of those gains.  As a result, facilities at Fermilab and the universities have been increasingly utilized, and resources have been much more full than in previous quarters, while site performance has remained just as good.  End-of-year hardware purchases will help to keep up with the increasing demands. 

Date            Milestone  
--------------  ------------------------------------------------
July 2015       Production release CMSSW_7_5_0 made available 
August 2015     Implement continuous integration for WMAgent
September 2015  Tier-1 purchases completed for 2015

Table: Major milestones achieved this quarter

## Fermilab Facilities

Run 2 data taking continued through the quarter. The Fermilab facility received over a petabyte of LHC collision data so far. Utilization of the Tier-1 has increased, with the Fermilab facility being full to capacity becoming the norm.  Site readiness improved over the previous quarter, and Fermilab passed CMS site tests 98% of the time for the quarter, as shown in Figure \ref{sc:f1}.  

![Fermilab readiness metrics for 2015Q3. Green indicates a passing metric, red a failed metric. Blue indicates a scheduled downtime. Fermilab was 98% available during the quarter.\label{sc:f1}][image-1]


The FY15 hardware purchase cycle was completed this quarter. Hardware was beginning to arrive and be installed late September. We were able to take advantage of very good market pricing this year both for disk and CPU, and used this as an opportunity to buy ahead for the next years, when significant additional budget pressure is expected from the HL-LHC upgrade program.  Fermilab purchased 168 nodes of 48-core Intel CPU batch nodes, and approximately 8 PB of disk storage toward the 2016, and to some extent 2017, CMS pledges. The increased size of the batch farm required upgrades in network connectivity to the storage pools, and management reserve funds were used to facilitate these procurements.  About 6 PB of tape media was purchased and installed to hold incoming Run 2 data.  


## University Facilities

This quarter saw a continuing increase in the use of the U.S. CMS Tier-2 facilities as Run 2 continued, as shown in Figure \ref{sc:f2}. This increase was largely due to running part of the  CMS data reconstruction at U.S. Tier-2 sites.  This workflow has been possible at Tier-2 sites since May, thanks in part to development work undertaken during LS1.  These workflows place a heavy strain on the internal networking capabilities of the sites, but our sites are all able to handle the increased load. Physics analysis with CRAB3 is also increasing.



The seven U.S. sites are planning or are in the process of finalizing their hardware purchases for 2015. The connection of the Tier-2 sites to the LHCONE VPN by ESNet is proceeding in an orderly manner and should be completed this quarter. All sites have deployed the HTCondor-CE computing element, and have either retired or are planning to retire their GRAM CEs very soon.



All of the U.S. CMS Tier-2 sites have operated successfully this quarter. On our two official performance metrics based on CMS test jobs, all sites were at least 92.4% available (a 4.4% improvement over the third quarter 2015) and 95% ready, 1% better than last quarter. The CMS goal for each of these metrics is 80%. The U.S. CMS Tier-2 centers delivered 44% of all computing time by Tier-2 sites in CMS (goal is \>25%), being 7 of the 8 top most-used Tier-2 sites in all of global CMS.

![Wall clock time consumption of CMS workflows at the U.S.CMS Tier-2 sites by month.\label{sc:f2}][image-2]



Fourteen Tier-3 sites required assistance from the Tier-3 support team this past quarter.  These support activities include helping sites complete the transition from OSG 3.1 to 3.2, including a critical security patch for GUMS that could only be applied to sites that have already upgraded. In addition, the site support team  continues to assist several sites in rebuilding their site in preparation for Run 2.  In particular, there is an ongoing effort to implement a modern cluster configuration and provisioning system at the University of Maryland, as well as new efforts to rebuild clusters at Rice and FSU.  Efforts continue to refresh documentation for site configuration and administration.  

Progress on CMS Connect has focused on connecting the CMS Connect infrastructure to CMS monitoring, which has required significant software development.  Documentation is nearly complete and beta testing should begin during the next quarter.

## Operations

In this quarter, the Tier-0 was operating in data-taking mode to process data from global data taking.  The prompt processing of the freshly recorded data was successful and proceeded without any remarkable problems.  Smaller issues and newly appearing software were quickly resolved and adapted easily enough that one could call it “operations as usual.”  We are preparing to change our present co-team leader for the Tier-0 operations (Dirk Hufnagel). He is a very valuable resource as he is also the core developer for the Tier-0 software we are now entering an operations-dominated period.



The activities in Monte Carlo production and Monte Carlo and data reprocessing have been rather limited and it was easy to keep up with the incoming requests. The streamlining of the production and processing activities has continued and the operations team was very well prepared to deal with the campaigns which started in the beginning of October. In particular, the integration of the dynamic data management tool for the space occupied by production data samples will be very valuable to reduce the worry about overstepping quotas. In the 4th quarter of FY2015 we have completed 2.44B DIGI-RECO events, 3.38B GEN-SIM events and redone 50M MINIAOD. The last number is expected to increase dramatically in the next quarter as we are have just started the re-reconstruction campaign. During this quarter we have not made significant progress to run production requests on the opportunistic HLT resource as data taking has been the priority. We hope to pick up this topic in the next quarter.


In the area of data transfers and data management substantial progress has been made. We have continued the integration of AAA operations and while there are still issues reported regularly the operations team is getting more comfortable with solving problems. The Dynamic Data Management has finally been deployed to carry out automatic deletions in the production data space. The strategy does not require a last copy but instead protects data from deletion by a simple locking mechanism. Any data (dataset or block level) that is in use by production at a given site has to be explicitly locked at the specific site. Thus any production activity that requires data will have to set its locks and release them once done. Many of those locks are extracted automatically from the production tools while others are entered explicitly by existing tools or by hand. The seamless integration of the dynamic data management into all production activities is expected to continue for a while before it can be considered complete. In the next quarter we expect to review the dynamic data management policies and continue development towards further optimizing space usage. Tape pledges at the Tier-1 centers have been reviewed and we are continuing our deletion campaigns to free up space on the existing tapes. The dynamic data management tools are a good candidate to to help reduce the effort used on the deletion campaigns.

## Computing Infrastructure and Services


This was the first quarter of Run 2 data taking. The work within U.S. CMS Computing Infrastructure and Services centered around supporting the data taking Run 2. We also continue to lay the groundwork for future improvements to come during and after Run 2. All projects have made progress on modernizing their Python code.  


During the quarter, work on the WMAgent concentrated on continued small improvements for Run 2 and the addition of new workflow types and features to support running on Amazon Web Services (AWS), including a new chained workflow type. WMAgent completed a campaign to modernize its Python code and restored automated testing of proposed changes in continuous integration. A gradual transition to Request Manager v2 continues and will be complete by FY16Q2.


With development effectively finished for the Tier-0, the aim for FY16Q1 and beyond is to reliably operate the system during Run 2 data taking. During Q4, the Tier-0 began to produce the MINIAOD data tier.  The Tier-0 was also updated to support automatic data management.  

Development responsibilities for CRAB3, begun by the U.S., have been almost completely shifted to international CMS. CRAB3 usage continues to show strong growth in at the expense of CRAB2 usage.  


DAS and DBS, the CMS metadata services, served larger and larger numbers of requests as the run began. A couple of changes enable this larger load. On the DAS side, horizontal scaling to additional servers has been implemented. DBS struggled with memory consumption issues that are being solved with Python generators. This will be moved into production in FY16Q1.  

There was one major GlideinWMS release during the quarter and two patch releases. The major release provides an important improvement for CMS: the ability to schedule an additional high-IO job in a multicore pilot. This is crucial for the Tier-0 to avoid scheduling many high-IO jobs on a single physical machine or leaving resources unallocated. This will be put into production in FY16Q1. In the future, GlideinWMS will improve the monitoring related to completed multicore glideins.  Coupled with the AWS pilot project, the GlideinWMS team will add native configuration support for EC2 spot pricing and availability zones in the glidein factory.  


Efforts continue to increase the operational robustness of the “Any Data, Anytime, Anywhere” infrastructure.  A number of issues in the stability of the regional redirectors were identified and fixed; along the way, many sites were pushed to upgrade to newer versions of Xrootd, which had a number of improvements.  Further improvements to monitoring and testing are being considered, and the transitional federation for less-performant sites will be fully populated in the coming quarter.



We are continuing to make progress on cloud and opportunistic resource usage. We completed our Gordon allocation, using 1.8M core hours on Gordon, roughly equivalent to 4M core hours at CMS T2 centers.  This allocation was used for GEN-SIM as well as DIGI-RECO. For the latter, the pileup samples were read via XRootd from UCSD-T2, and the GEN-SIM input for DIGI-RECO were read via Xrootd from FNAL. Output was staged out directly to FNAL. Merging was run at FNAL, i.e. contrary to the previous use of Gordon in 2013, no significant disk space at Gordon was used, simplifying operations on Gordon significantly. 

For cloud we rely on the HEPCloud project at FNAL to execute our AWS project. For our future use of NERSC and Comet, we are relying on OSG to work through the issues of using these resources. In both cases we collaborate with FNAL and OSG as needed to make progress. The goal in both cases is to establish our capability to elastically grow resources to meet deadlines.



## Software and Support

The U.S. CMS Software and Support group continues to contribute to the production of improved software releases to support the development of the major fronts of CMS. The CMSSW_7_4_X series is being maintained as the main production release for operating the HLT trigger and prompt reconstruction for the 2015 run.  The introduction of the multi-threaded reconstruction has resulted in large memory savings and reduced processing latency enabling the handling of the large Run 2 trigger rates.  As part of the commissioning and deployment of the reconstruction application, it was discovered that the special configuration used in the Tier-0, which requires many output modules and just-in-time compilation within ROOT, triggered larger concurrency inefficiencies compared to the testing configurations that had been used during the development of the threaded application.  These problems were fixed in the development release series CMSSW\_7\_6\_X, which will be used for the end-of-year reprocessing.  Maintenance work was also done on the 7\_4 release to allow it to be used in the HLT, which is currently running in single threaded mode. Due to a lack of sufficient memory per core it cannot sustain the required trigger rate during the last two weeks of data taking without moving to multi-threaded processing.  In Figure \ref{sc:f3}, the point circled in black marks the current limit of the single-threaded HLT application, and the green circle marks the rate that the multi-threaded application allows. This welcomed capability comes just in time to maximize physics data taking efficiency for CMS.

![Event throughput rates as a function of the number of threads for single- and multi-threaded applications.\label{sc:f3}][image-3]


Meanwhile the CMSSW 7_5_X release series is being prepared to operate during the heavy-ion run.  In this release the GEN-SIM application has been validated to run in multi-threaded mode.

[image-1]:	figures/image02.png
[image-2]:	figures/image01.jpg
[image-3]:	figures/image00.png
