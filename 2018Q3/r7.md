\clearpage

# Software and Computing

During this quarter, as the LHC moved towards the end of proton collisions for Run 2, the Software & Computing group made progress on supporting analysis of Run 2 data while also making preparations for Run 3 and beyond. Utilization of facilities remained high (modulo a late-summer vacation dip), as the facilities themselves continued their excellent performance. Of particular note for the facilities is that the Fermilab Tier-1 center has received its new tape library, thus beginning the transition to a new tape technology. Computing operations continued to deliver samples needed for physics analysis, while also working through some operational problems that arose, as detailed below. A number of development efforts are taking shape as CMS moves away from home-grown solutions and towards community projects, both in the short term (with the deployment of the CRIC site information system and the overhaul of the CMSWeb platform) and longer term (with the choice of Rucio as the data management system for Run 3 and beyond, and first steps towards its deployment). A variety of activities are enabling the use of non-traditional architectures and non-traditional facilities, such as those at HPC centers.

### Major Milestones Achieved this Quarter

--------------------------------------------------------------------------------
Date    Milestone
------- ------------------------------------------------------------------------
1 Jul   Complete Rucio functional review in advance of CMS DM review

5 Jul   New Fermilab tape buffer disk nodes deployed in production

1 Jul   C++ Modules integration in CMSSW

1 Jul   Functioning CPU version of Machine Learning benchmark

1 Aug   Create CMSSW interface producer for tracking on advanced architectures

1 Aug   Allow concurrent Run transitions in multi-threaded CMSSW

1 Aug   "Pixel tracking \@GPU" integrated/prototyped in CMSSW

1 Aug   "Parallel kalman filter" integrated/prototyped in CMSSW

15 Aug  TS4500 tape robot delivery and assembly at FNAL

1 Sep   Migration and cleanup of CMS HEP environment script to EOS/CVMFS

17 Sep  2018 FNAL CPU purchase awarded

30 Sep  Use CMS portion of initial allocation at NERSC

1 Oct   Working GPU version of Machine Learning benchmark

1 Oct   uproot-writing, awkward-arrays in uproot published as uproot

--------------------------------------------------------------------------------

## Fermilab Facilities

During this quarter the LHC proton-proton run continued and the Fermilab Facilities provided reliable custodial storage, processing, and analysis resources to U.S. CMS collaborators.  The site was well utilized, with the facility providing 43.0 million wall-clock hours of processing to CMS.

![Fermilab readiness metrics for 2018Q3. Green indicates a passing metric, red a failing metric, and blue a scheduled downtime. The height of a red bar indicates how much of the day services were affected. Black and yellow across the bottom indicates LHC machine running. This quarter LHC continued proton-proton physics running. Fermilab passed metrics 98.1% during the quarter.](figures/image1.png){#fig:t1}

Figure {@fig:t1} shows the site readiness metrics for the quarter. The Tier 1 facility passed CMS site availability metrics 98.1% of the time. The blue entries in the figure indicate a scheduled downtime in July for work needed on the power infrastructure in the Feynman Computing Center. There were two periods of failing metrics during the quarter: In August a bug in dCache triggered by a security-mandated move to TLS 1.2 encryption resulted in failed data transfers to several sites. In September a bad fiber on the incoming 100Gb network link led to reduced incoming transfer throughput.

At the beginning of the quarter FNAL received new storage nodes to replace equipment being retired out of the tape dCache pool. The dCache pool serves as the disk buffer in front of the tape system, and these nodes were successfully introduced into production without downtime. In September the PO for the batch worker nodes replacing retiring workers was awarded, with delivery expected late October/early November.

Towards the end of the quarter Fermilab received delivery of new TS4500 tape library and began commissioning. Legal conflicts between suppliers of LTO8 media resulted in worldwide unavailability of the LTO8 tape media. A decision was made to initially move to the lower capacity M8 (reformatted LTO7) cartridges to be able to commission the robot before the end of the LHC run. That media arrived on October 2, allowing commissioning of the new system to proceed in earnest during the coming quarter.


## University Facilities

As seen in Figure @fig:t2, CMS production and analysis activities this quarter continued to run at high levels approaching full capacity.  August is typically a slower month for CMS computing due to conferences and holidays. There were also issues with central production during September which affected usage.

All of the U.S. CMS Tier-2 sites operated successfully last quarter. On our two official performance metrics based on CMS test jobs, all sites were at least 96%
"[available](https://www.google.com/url?q=http://wlcg-sam-cms.cern.ch/templates/ember/%23/historicalsmry/heatMap?end_time%3D2018%252F10%252F01%252000%253A00%26granularity%3DDaily%26profile%3DCMS_CRITICAL%26site%3DT2_US_Caltech%252CT2_US_Florida%252CT2_US_MIT%252CT2_US_Nebraska%252CT2_US_Purdue%252CT2_US_UCSD%252CT2_US_Wisconsin%26start_time%3D2018%252F07%252F01%252000%253A00%26time%3Dmanual%26type%3DAvailability%2520Ranking%2520Plot&sa=D&ust=1545931895812000)"
and 88%
"[ready](https://www.google.com/url?q=https://dashb-ssb.cern.ch/dashboard/request.py/sitereadinessrank?columnid%3D234%23time%3Dcustom%26start_date%3D2018-07-01%26end_date%3D2018-10-01%26sites%3Dmultiple%26timebins%3Dfalse%26nodata%3Dfalse%26binsselect%3Ddefault%26clouds%3Dall%26site%3DT2_US_Caltech,T2_US_Florida,T2_US_MIT,T2_US_Nebraska,T2_US_Purdue,T2_US_UCSD,T2_US_Wisconsin&sa=D&ust=1545931895812000)".
The CMS requirement for each of these metrics is 80%, but the U.S. CMS performance goal is 90%, which all sites met except for Caltech, which had a series of downtimes due to unrelated technical problems during September. Nonetheless our commitments to CMS were met with success. The U.S. CMS Tier-2 centers delivered
[48.2%](https://www.google.com/url?q=http://dashb-cms-jobsmry.cern.ch/dashboard/request.py/consumptions_individual?sites%3DT2_AT_Vienna%26sites%3DT2_BE_IIHE%26sites%3DT2_BE_UCL%26sites%3DT2_BR_SPRACE%26sites%3DT2_BR_UERJ%26sites%3DT2_CH_CSCS%26sites%3DT2_CN_Beijing%26sites%3DT2_DE_DESY%26sites%3DT2_DE_DESY_Test%26sites%3DT2_DE_RWTH%26sites%3DT2_EE_Estonia%26sites%3DT2_EE_Estonia_Test%26sites%3DT2_ES_CIEMAT%26sites%3DT2_ES_IFCA%26sites%3DT2_FI_HIP%26sites%3DT2_FI_HIP_Test%26sites%3DT2_FR_CCIN2P3%26sites%3DT2_FR_GRIF_IRFU%26sites%3DT2_FR_GRIF_LLR%26sites%3DT2_FR_IPHC%26sites%3DT2_GR_Ioannina%26sites%3DT2_HU_Budapest%26sites%3DT2_IN_TIFR%26sites%3DT2_IT_Bari%26sites%3DT2_IT_Legnaro%26sites%3DT2_IT_LegnaroTest%26sites%3DT2_IT_Pisa%26sites%3DT2_IT_Rome%26sites%3DT2_KR_KNU%26sites%3DT2_MY_UPM_BIRUNI%26sites%3DT2_PK_NCP%26sites%3DT2_PL_Swierk%26sites%3DT2_PL_Warsaw%26sites%3DT2_PT_NCG_Lisbon%26sites%3DT2_RU_IHEP%26sites%3DT2_RU_INR%26sites%3DT2_RU_ITEP%26sites%3DT2_RU_JINR%26sites%3DT2_RU_PNPI%26sites%3DT2_RU_RRC_KI%26sites%3DT2_RU_SINP%26sites%3DT2_TH_CUNSTDA%26sites%3DT2_TR_METU%26sites%3DT2_UA_KIPT%26sites%3DT2_UK_London_Brunel%26sites%3DT2_UK_London_BrunelTest%26sites%3DT2_UK_London_IC%26sites%3DT2_UK_SGrid_Bristol%26sites%3DT2_UK_SGrid_RALPP%26sites%3DT2_US_Caltech%26sites%3DT2_US_Florida%26sites%3DT2_US_MIT%26sites%3DT2_US_Nebraska%26sites%3DT2_US_Purdue%26sites%3DT2_US_UCSD%26sites%3DT2_US_Vanderbilt%26sites%3DT2_US_Wisconsin%26sitesSort%3D2%26start%3D2018-07-01%26end%3D2018-10-01%26timeRange%3Ddaily%26granularity%3DMonthly%26generic%3D0%26sortBy%3D0%26series%3DAll%26type%3Dewa&sa=D&ust=1545931895813000)
of all computing time by Tier-2 sites in CMS last quarter, approximately unchanged from the previous quarter.

Steady progress to complete milestones and upgrades at the Tier-2 sites is being made. Generally these fall into a few broad categories of upgrading to SL7, IPv6, and simplifying the architecture of the sites.  Https-based transfers are now possible between several sites.

![How processing resources were used at the U.S. CMS Tier-2 sites, by month. ](figures/image3.png){#fig:t2}

The U.S. CMS Tier-3 support team provided help to thirteen sites on a number of issues mainly related to OSG software upgrades, PhEDEx transfers, XRootD, and basic systems administration. CMS Connect also required some work to address dashboard issues and to create a new EL7 login node to prepare for the eventual retirement of the EL6 one. Work also began on allowing interactive GPU access via CMS Connect. The team tracked issues with GGUS instead of OSG tickets. There were two Tier-3 related presentations made by team members at CHEP.

## Computing Operations

Digitizing and simulating Monte Carlo events for the 2017 data analyses was again the largest processing activity in the quarter. About 5 billion events were processed, the remaining third of the requested Monte Carlo events.

In July small numbers of Monte Carlo events for the 2018 data analyses were produced with updated software versions. In September generation of the premixing library with the final software for the 2018 detector configuration started. This was completed later that month and Monte Carlo event production using the premix library started. So far 200 million events have been processed. About three quarters of the current requests will use the resource-efficient, pre-mixing pile-up method, which takes advantage of computing technologies developed in the US.

Data processing was impacted by three issues during the quarter:

1.  Storage space at CERN filled up due to temporary files from incomplete workflows. Small intermediate files from the processing step are merged into large multi-GB files for analysis and archiving. The available CPU at CERN from the HLT farm and Tier-0 occupancy had been overestimated by more than a factor of two. Consequently, too many workflows were assigned, requiring more space, progressing more slowly, and keeping the temporary space occupied. In addition the large number of workflows in the system and the non-negligible failure rate prevented workflows from finishing. No new workflows were assigned to CERN for the next month and a half to work down the processing backlog and clear the space used for unmerged files.

2.  Misconfigured workflows caused a huge number of very short jobs (only minutes long instead of several hours) that brought the global HTCondor pool to its limits. For about two weeks we were not able to utilize all the compute resources available to CMS.

3. Instabilities of the CERN EOS storage system caused processing interrupts and operator interventions. Various issues were in play: fuse-mount instabilities due to IP-based authorization and enabling IPv6, files getting "lost" during namespace compacting, permission issues, and self-inflicted namespace overload. For now EOS is behaving stably again but it is not entirely clear whether all issues have been addressed.

The Tier-0 reformatted and reconstructed detector data as they were recorded. Two LHC machine development periods fell into this quarter. During the September machine development period, transfer tests between the experiment at P5 and the CERN EOS storage system were done in preparation for the heavy-ion data recording in the following quarter. The performance goal was achieved on the second day of testing.

500 million events of the parked data for B physics (out of 9.8 billion recorded so far) were passed through the prompt reconstruction process end of August.

![Numbers of events produced from different categories of workflows during 2018.](figures/image2.png){#fig:co}

## Computing Infrastructure and Services

WMAgent has implemented changes related to the dataset parentage which have been fixed at several levels. This was the last obstacle to widespread usage of StepChain workflow types, which are well suited to HPC running. In addition, the WMAgent development team has also implemented the CRIC client APIs in WMCore; these are already being used by an agent in production and some analysis services. CRIC is a WLCG supported replacement for the CMS-only project SiteDB. SiteDB is scheduled to be fully retired in the first quarter of 2019.

We continue to contribute to the overhaul of CMSWeb led by CERN. In this quarter, we demonstrated CMSWeb could be converted to a Kubernetes-based infrastructure with no CMS-specific frontend to maintain and each backend component converted to Docker.  Work remains to be done to integrate CMS's authentication scheme; we plan to continue to improve this prototype with a goal of replacing the current CMSWeb platform in the first quarer of 2019.

After a comprehensive review, CMS has chosen Rucio as the next generation CMS data management product. The transition to Rucio has begun with the setup of a CMS instance of Rucio on CERN hardware using the Kubernetes stack. The transition (development, integration, and corresponding cross-coordination) has begun to be planned out in detail.  The current goal for full transition from the existing Dynamo/PhEDEx system at the start of 2020.

A variety of technical improvements have been made to the Xrootd infrastructure used by CMS. For example, we have improved the cache purge algorithm to also take into account file-usage. This is needed to be able to operate several cache instances on the same node and for serverless caching (e.g., on laptops, desktops). We have also implemented new features prerequisite to integrating the Xrootd-based cache with Rucio. Finally, improvements have been made to HTTPS-based third party copy in Xrootd, allowing us to perform copies between sites without GridFTP. This helps U.S. CMS (along with OSG) mitigate the risk from the Globus Toolkit's end-of-support.

We commissioned the Bridges HPC at PSC for CMS and made it directly usable for CMS production workflows. Using Rucio, we transferred the new Fall 18 version of the pileup library to make it locally available at NERSC. This allowed scaling up to about 25k cores in use at NERSC for CMS production workflows. By the end of reporting period, we had used nearly all the CMS share of the original 50 Mhour allocation at NERSC as well as the CMS allocation at Bridges.

## Software and Support

We finalized the integration of the production releases for MC and data re-miniAOD of 2016 data, main 2018 data and MC processing and 2018 heavy-ions data taking. Progress was made on making CMSSW C++17 standard compliant and building the visualization on macOS using system compilers.

Work continued on allowing concurrent event processing in multi-threading mode of CMSSW. We concentrated on run and luminosity block transitions and IOVs (intervals of validity). We also investigated the usage of OpenMP 4.5 task APIs in our CMSSW toy framework and started comparisons to our current TBB implementation, discussed with HPC experts at the CCE Scalable I/O workshop. We made a lot of progress establishing a ML suite for benchmarking purposes, by having now the first GPU based benchmark application available.

In addition to maintenance of the current and near-term geometries of the detector, we made progress in transitioning to the community geometry description solution DD4HEP. The community foundation of our visualization package FireWorks, EvE, was evolved into the ROOT7 era and is ready to be included into the December ROOT release. We also made progress on the client-server implementation of our visualization.

Kalman filter tracking on advanced architectures achieved additional speed-ups on Skylake Gold architectures while retaining the efficiency and purity of the non-parallel Kalman filter implementation. The GPU pixel tracking R&D integration in CMSSW made progress by prototyping the use of CUDA from the framework.

Together with DIANA-HEP, we enabled uproot file writing support for numpy arrays and other types for interoperability of the HEP analysis environment with industry tools. Ten CHEP contributions featured topics whose maintenance, integration into production and scaling to higher scales were supported by the operations program.

## Other activities

### Blueprint

The Blueprint activity continues to focus on computing model evolutions and discussions as a means to identify research and development for HL-LHC. We organized five U.S. CMS blueprint meetings this quarter. These included: initial results from a HL-LHC CMS cost model; results from the Big Data Express project at Fermilab, Oak Ridge, and KISTI; concepts for future analysis facilities; design and results from a caching system at UCSD; and initial results from the U.S. CMS machine learning benchmark design project.

The Blueprint team is an active participant in the CMS ECOM2X working group. Our modeling efforts will be used as a basis for discussion on how to evolve the CMS computing and analysis model towards HL-LHC. We have organized initial presentations on our work during the quarter. Blueprint modeling work and results were also presented in a talk at the CHEP conference. We are also actively engaged in the WLCG DOMA working groups to advance the WLCG HL-LHC strategy discussions. In particular, U.S. CMS co-leads two of the three WLCG DOMA working groups, one on transfer protocols and one on data access patterns and caching.

### Security task force

The security team documented U.S. CMS site security responsibilities and shared the document amongst the site administrators. This document provides a set of responsibilities that every site in the U.S. CMS collaboration must comply with. A list of security policies and procedures from WLCG, OSG and EGI is included in its appendix.

The team also verified and updated the U.S. CMS Incident Response procedure to be aligned with the Global CMS Incident Response procedure, and developed an internal procedure for incident response for the U.S.  CMS security task force. Training remains a priority for the security team. It prepared training material for incident response and job traceability, and conducted three training sessions with 17 participants from 10 Tier-2 sites (including the seven U.S. HEP sites, the nuclear physics site at Vanderbilt, and two sites in Brazil). A separate session shared the training and the site security responsibilities with Fermilab site administrators.

